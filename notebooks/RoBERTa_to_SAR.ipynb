{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RoBERTa to SAR",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Masum06/Text2App/blob/master/notebooks/RoBERTa_to_SAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F864UIM1YUEq"
      },
      "source": [
        "Code borrowed from: https://github.com/microsoft/CodeXGLUE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nc12sfRZSmxT",
        "outputId": "278e079e-ecbf-4f94-a422-4043a846a637"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jul  1 09:19:27 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8    12W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBG3u_eYQHKY",
        "outputId": "80f6ca31-2dcc-470e-d6eb-bacd1fbf2b87"
      },
      "source": [
        "!pip install transformers==4.5.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==4.5.0 in /usr/local/lib/python3.7/dist-packages (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (4.5.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (0.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (0.0.45)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (2019.12.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.0) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHNqpkh3Q2MG",
        "outputId": "cdf510aa-7ebc-4fb8-c0ab-e358200ab243"
      },
      "source": [
        "!git clone https://github.com/Masum06/Text2App.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Text2App'...\n",
            "remote: Enumerating objects: 550, done.\u001b[K\n",
            "remote: Counting objects: 100% (153/153), done.\u001b[K\n",
            "remote: Compressing objects: 100% (117/117), done.\u001b[K\n",
            "remote: Total 550 (delta 98), reused 64 (delta 35), pack-reused 397\u001b[K\n",
            "Receiving objects: 100% (550/550), 241.85 MiB | 42.60 MiB/s, done.\n",
            "Resolving deltas: 100% (180/180), done.\n",
            "Checking out files: 100% (227/227), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRLNH4uJG7rI"
      },
      "source": [
        "# Text2App"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jd61C2XOiyd"
      },
      "source": [
        "Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "v6ziTZ3aTflj",
        "outputId": "796a7449-06c9-4641-e339-b7aa68990c5d"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG59i7w5RCu_",
        "outputId": "48ff18d6-a2fe-4069-d19c-936f672b1110"
      },
      "source": [
        "cd Text2App/training_RoBERTa/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Text2App/training_RoBERTa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx5HeX1IZK17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cf66ec6-9e2a-43a8-9bfe-3555fb3b3476"
      },
      "source": [
        "# For saving checkpoint to Google Drive while working on Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yud9saHSVRi"
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv('../synthesized_data/nl_sar_train.csv')\n",
        "dev = pd.read_csv('../synthesized_data/nl_sar_valid.csv')\n",
        "test = pd.read_csv('../synthesized_data/nl_sar_test.csv')\n",
        "unseen_test = pd.read_csv('../synthesized_data/unseen_test.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0AwB1I6Yhle"
      },
      "source": [
        "class MyTokenizer:\n",
        "  vocab_size = 0\n",
        "  vocab = []\n",
        "  id_to_token = {}\n",
        "  token_to_id = {}\n",
        "\n",
        "  def __init__(self):\n",
        "    self.vocab = list(set(\" \".join(list(train['SAR'])+list(dev['SAR'])+list(test['SAR'])).split()))\n",
        "    self.vocab.sort()\n",
        "    self.vocab_size = len(self.token_to_id) \n",
        "    # Special tokens: <s><pad></s><unk>\n",
        "    self.add_token('<s>')\n",
        "    self.add_token('<pad>')\n",
        "    self.add_token('</s>')\n",
        "    self.add_token('<unk>')\n",
        "    for v in self.vocab:\n",
        "      self.add_token(v)\n",
        "    self.add_token('None')\n",
        "\n",
        "  def tokenize(self, s):\n",
        "    return s.split()\n",
        "\n",
        "  def add_token(self, s):\n",
        "    if s not in self.token_to_id:\n",
        "      self.id_to_token[self.vocab_size] = s\n",
        "      self.token_to_id[s] = self.vocab_size\n",
        "      self.vocab_size+=1\n",
        "\n",
        "  def convert_string_to_ids(self, s):\n",
        "    tokens = s.split()\n",
        "    ids = []\n",
        "    for token in tokens:\n",
        "      ids.append(self.token_to_id[token])\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \"\"\n",
        "    for id in ids:\n",
        "      text += self.id_to_token[id] + \" \"\n",
        "    return text[:-1]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLOsuOtfPByQ"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VbExmvP0-Aw"
      },
      "source": [
        "# Copyright (c) Microsoft Corporation. \n",
        "# Licensed under the MIT license.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import copy\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "        Build Seqence-to-Sequence.\n",
        "        \n",
        "        Parameters:\n",
        "        * `encoder`- encoder of seq2seq model. e.g. roberta\n",
        "        * `decoder`- decoder of seq2seq model. e.g. transformer\n",
        "        * `config`- configuration of encoder model. \n",
        "        * `beam_size`- beam size for beam search. \n",
        "        * `max_length`- max length of target for beam search. \n",
        "        * `sos_id`- start of symbol ids in target for beam search.\n",
        "        * `eos_id`- end of symbol ids in target for beam search. \n",
        "    \"\"\"\n",
        "    def __init__(self, encoder,decoder,config,beam_size=None,max_length=None,sos_id=None,eos_id=None):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder=decoder\n",
        "        self.config=config\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(2048, 2048)))\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.lm_head = nn.Linear(config.hidden_size, decoder_tokenizer.vocab_size, bias=False) #config.vocab_size\n",
        "        self.lsm = nn.LogSoftmax(dim=-1)\n",
        "        # self.tie_weights()\n",
        "        \n",
        "        self.beam_size=beam_size\n",
        "        self.max_length=max_length\n",
        "        self.sos_id=sos_id\n",
        "        self.eos_id=eos_id\n",
        "        \n",
        "    def _tie_or_clone_weights(self, first_module, second_module):\n",
        "        \"\"\" Tie or clone module weights depending of weither we are using TorchScript or not\n",
        "        \"\"\"\n",
        "        if self.config.torchscript:\n",
        "            first_module.weight = nn.Parameter(second_module.weight.clone())\n",
        "        else:\n",
        "            first_module.weight = second_module.weight\n",
        "                  \n",
        "    def tie_weights(self):\n",
        "        \"\"\" Make sure we are sharing the input and output embeddings.\n",
        "            Export to TorchScript can't handle parameter sharing so we are cloning them instead.\n",
        "        \"\"\"\n",
        "        self._tie_or_clone_weights(self.lm_head,\n",
        "                                   self.encoder.embeddings.word_embeddings)        \n",
        "        \n",
        "    def forward(self, source_ids=None,source_mask=None,target_ids=None,target_mask=None,args=None):   \n",
        "        outputs = self.encoder(source_ids, attention_mask=source_mask)\n",
        "        encoder_output = outputs[0].permute([1,0,2]).contiguous()\n",
        "        if target_ids is not None:  \n",
        "            attn_mask=-1e4 *(1-self.bias[:target_ids.shape[1],:target_ids.shape[1]])\n",
        "            tgt_embeddings = self.encoder.embeddings(target_ids).permute([1,0,2]).contiguous() ## MH: Problmatic\n",
        "            out = self.decoder(tgt_embeddings,encoder_output,tgt_mask=attn_mask,memory_key_padding_mask=(1-source_mask).bool())\n",
        "            hidden_states = torch.tanh(self.dense(out)).permute([1,0,2]).contiguous()\n",
        "            lm_logits = self.lm_head(hidden_states)\n",
        "            # Shift so that tokens < n predict n\n",
        "            active_loss = target_mask[..., 1:].ne(0).view(-1) == 1\n",
        "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
        "            shift_labels = target_ids[..., 1:].contiguous()\n",
        "            # Flatten the tokens\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1))[active_loss],\n",
        "                            shift_labels.view(-1)[active_loss])\n",
        "\n",
        "            outputs = loss, loss*active_loss.sum(), active_loss.sum()\n",
        "            # print(\"Inside forward, outputs:\", outputs)\n",
        "            return outputs\n",
        "        else:\n",
        "            #Predict \n",
        "            preds=[]       \n",
        "            zero=torch.cuda.LongTensor(1).fill_(0)     \n",
        "            for i in range(source_ids.shape[0]):\n",
        "                context=encoder_output[:,i:i+1]\n",
        "                context_mask=source_mask[i:i+1,:]\n",
        "                beam = Beam(self.beam_size,self.sos_id,self.eos_id)\n",
        "                input_ids=beam.getCurrentState()\n",
        "                context=context.repeat(1, self.beam_size,1)\n",
        "                context_mask=context_mask.repeat(self.beam_size,1)\n",
        "                for _ in range(self.max_length): \n",
        "                    if beam.done():\n",
        "                        break\n",
        "                    attn_mask=-1e4 *(1-self.bias[:input_ids.shape[1],:input_ids.shape[1]])\n",
        "                    tgt_embeddings = self.encoder.embeddings(input_ids).permute([1,0,2]).contiguous()\n",
        "                    out = self.decoder(tgt_embeddings,context,tgt_mask=attn_mask,memory_key_padding_mask=(1-context_mask).bool())\n",
        "                    out = torch.tanh(self.dense(out))\n",
        "                    hidden_states=out.permute([1,0,2]).contiguous()[:,-1,:]\n",
        "                    out = self.lsm(self.lm_head(hidden_states)).data\n",
        "                    beam.advance(out)\n",
        "                    input_ids.data.copy_(input_ids.data.index_select(0, beam.getCurrentOrigin()))\n",
        "                    input_ids=torch.cat((input_ids,beam.getCurrentState()),-1)\n",
        "                hyp= beam.getHyp(beam.getFinal())\n",
        "                pred=beam.buildTargetTokens(hyp)[:self.beam_size]\n",
        "                pred=[torch.cat([x.view(-1) for x in p]+[zero]*(self.max_length-len(p))).view(1,-1) for p in pred]\n",
        "                preds.append(torch.cat(pred,0).unsqueeze(0))\n",
        "                \n",
        "            preds=torch.cat(preds,0)\n",
        "            # print(\"Inside Forward, preds: \", preds)\n",
        "            return preds   \n",
        "        \n",
        "        \n",
        "\n",
        "class Beam(object):\n",
        "    def __init__(self, size,sos,eos):\n",
        "        self.size = size\n",
        "        self.tt = torch.cuda\n",
        "        # The score for each translation on the beam.\n",
        "        self.scores = self.tt.FloatTensor(size).zero_()\n",
        "        # The backpointers at each time-step.\n",
        "        self.prevKs = []\n",
        "        # The outputs at each time-step.\n",
        "        self.nextYs = [self.tt.LongTensor(size)\n",
        "                       .fill_(0)]\n",
        "        self.nextYs[0][0] = sos\n",
        "        # Has EOS topped the beam yet.\n",
        "        self._eos = eos\n",
        "        self.eosTop = False\n",
        "        # Time and k pair for finished.\n",
        "        self.finished = []\n",
        "\n",
        "    def getCurrentState(self):\n",
        "        \"Get the outputs for the current timestep.\"\n",
        "        batch = self.tt.LongTensor(self.nextYs[-1]).view(-1, 1)\n",
        "        return batch\n",
        "\n",
        "    def getCurrentOrigin(self):\n",
        "        \"Get the backpointers for the current timestep.\"\n",
        "        return self.prevKs[-1]\n",
        "\n",
        "    def advance(self, wordLk):\n",
        "        \"\"\"\n",
        "        Given prob over words for every last beam `wordLk` and attention\n",
        "        `attnOut`: Compute and update the beam search.\n",
        "        Parameters:\n",
        "        * `wordLk`- probs of advancing from the last step (K x words)\n",
        "        * `attnOut`- attention at the last step\n",
        "        Returns: True if beam search is complete.\n",
        "        \"\"\"\n",
        "        numWords = wordLk.size(1)\n",
        "\n",
        "        # Sum the previous scores.\n",
        "        if len(self.prevKs) > 0:\n",
        "            beamLk = wordLk + self.scores.unsqueeze(1).expand_as(wordLk)\n",
        "\n",
        "            # Don't let EOS have children.\n",
        "            for i in range(self.nextYs[-1].size(0)):\n",
        "                if self.nextYs[-1][i] == self._eos:\n",
        "                    beamLk[i] = -1e20\n",
        "        else:\n",
        "            beamLk = wordLk[0]\n",
        "        flatBeamLk = beamLk.view(-1)\n",
        "        bestScores, bestScoresId = flatBeamLk.topk(self.size, 0, True, True)\n",
        "\n",
        "        self.scores = bestScores\n",
        "\n",
        "        # bestScoresId is flattened beam x word array, so calculate which\n",
        "        # word and beam each score came from\n",
        "        prevK = bestScoresId // numWords\n",
        "        self.prevKs.append(prevK)\n",
        "        self.nextYs.append((bestScoresId - prevK * numWords))\n",
        "\n",
        "\n",
        "        for i in range(self.nextYs[-1].size(0)):\n",
        "            if self.nextYs[-1][i] == self._eos:\n",
        "                s = self.scores[i]\n",
        "                self.finished.append((s, len(self.nextYs) - 1, i))\n",
        "\n",
        "        # End condition is when top-of-beam is EOS and no global score.\n",
        "        if self.nextYs[-1][0] == self._eos:\n",
        "            self.eosTop = True\n",
        "\n",
        "    def done(self):\n",
        "        return self.eosTop and len(self.finished) >=self.size\n",
        "\n",
        "    def getFinal(self):\n",
        "        if len(self.finished) == 0:\n",
        "            self.finished.append((self.scores[0], len(self.nextYs) - 1, 0))\n",
        "        self.finished.sort(key=lambda a: -a[0])\n",
        "        if len(self.finished) != self.size:\n",
        "            unfinished=[]\n",
        "            for i in range(self.nextYs[-1].size(0)):\n",
        "                if self.nextYs[-1][i] != self._eos:\n",
        "                    s = self.scores[i]\n",
        "                    unfinished.append((s, len(self.nextYs) - 1, i)) \n",
        "            unfinished.sort(key=lambda a: -a[0])\n",
        "            self.finished+=unfinished[:self.size-len(self.finished)]\n",
        "        return self.finished[:self.size]\n",
        "\n",
        "    def getHyp(self, beam_res):\n",
        "        \"\"\"\n",
        "        Walk back to construct the full hypothesis.\n",
        "        \"\"\"\n",
        "        hyps=[]\n",
        "        for _,timestep, k in beam_res:\n",
        "            hyp = []\n",
        "            for j in range(len(self.prevKs[:timestep]) - 1, -1, -1):\n",
        "                hyp.append(self.nextYs[j+1][k])\n",
        "                k = self.prevKs[j][k]\n",
        "            hyps.append(hyp[::-1])\n",
        "        return hyps\n",
        "    \n",
        "    def buildTargetTokens(self, preds):\n",
        "        sentence=[]\n",
        "        for pred in preds:\n",
        "            tokens = []\n",
        "            for tok in pred:\n",
        "                if tok==self._eos:\n",
        "                    break\n",
        "                tokens.append(tok)\n",
        "            sentence.append(tokens)\n",
        "        return sentence\n",
        "        \n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrH5YUx6IvWz"
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"\n",
        "Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\n",
        "GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\n",
        "using a masked language modeling (MLM) loss.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "import os\n",
        "import sys\n",
        "import bleu\n",
        "import pickle\n",
        "import torch\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "# import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from io import open\n",
        "from itertools import cycle\n",
        "import torch.nn as nn\n",
        "# from model import Seq2Seq\n",
        "from tqdm import tqdm, trange\n",
        "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
        "                          RobertaConfig, RobertaModel, RobertaTokenizer)\n",
        "MODEL_CLASSES = {'roberta': (RobertaConfig, RobertaModel, RobertaTokenizer)}\n",
        "\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class Example(object):\n",
        "    \"\"\"A single training/test example.\"\"\"\n",
        "    def __init__(self,\n",
        "                 idx,\n",
        "                 source,\n",
        "                 target,\n",
        "                 ):\n",
        "        self.idx = idx\n",
        "        self.source = source\n",
        "        self.target = target\n",
        "\n",
        "def read_examples(filename):\n",
        "    \"\"\"Read examples from filename.\"\"\"\n",
        "    examples=[]\n",
        "\n",
        "    ## open DF, convert to json, iterate one by one\n",
        "    # with open(filename,encoding=\"utf-8\") as f:\n",
        "    #     for idx, line in enumerate(f):\n",
        "    #         line=line.strip()\n",
        "    #         js=json.loads(line)\n",
        "    #         if 'idx' not in js:\n",
        "    #             js['idx']=idx\n",
        "    #         sar=' '.join(js['SAR']).replace('\\n',' ')\n",
        "    #         sar=' '.join(code.strip().split())\n",
        "    #         nl=' '.join(js['NL']).replace('\\n','')\n",
        "    #         nl=' '.join(nl.strip().split())  \n",
        "\n",
        "    df = pd.read_csv(filename)\n",
        "    data_list = list(df.T.to_dict().values())\n",
        "    for idx, data in enumerate(data_list):\n",
        "      nl = data['NL']\n",
        "      sar = data['SAR']\n",
        "      examples.append(\n",
        "          Example(\n",
        "                  idx = idx,\n",
        "                  source=nl,\n",
        "                  target=sar,\n",
        "                  ) \n",
        "      )\n",
        "    return examples\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single training/test features for a example.\"\"\"\n",
        "    def __init__(self,\n",
        "                 example_id,\n",
        "                 source_ids,\n",
        "                 target_ids,\n",
        "                 source_mask,\n",
        "                 target_mask,\n",
        "\n",
        "    ):\n",
        "        self.example_id = example_id\n",
        "        self.source_ids = source_ids\n",
        "        self.target_ids = target_ids\n",
        "        self.source_mask = source_mask\n",
        "        self.target_mask = target_mask       \n",
        "        \n",
        "\n",
        "\n",
        "def convert_examples_to_features(examples, tokenizer, args,stage=None): # MH: make it encoder_tokenizer, decoder_tokenizer\n",
        "    features = []\n",
        "    for example_index, example in enumerate(examples):\n",
        "        #source\n",
        "        source_tokens = tokenizer.tokenize(example.source)[:args.max_source_length-2]\n",
        "        source_tokens =[tokenizer.cls_token]+source_tokens+[tokenizer.sep_token]\n",
        "        source_ids =  tokenizer.convert_tokens_to_ids(source_tokens) \n",
        "        source_mask = [1] * (len(source_tokens))\n",
        "        padding_length = args.max_source_length - len(source_ids)\n",
        "        source_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "        source_mask+=[0]*padding_length\n",
        "\n",
        "        #target\n",
        "        if stage==\"test\":\n",
        "            target_tokens = ['None'] #tokenizer.tokenize(\"None\")\n",
        "        else:\n",
        "            # target_tokens = tokenizer.tokenize(example.target)[:args.max_target_length-2]\n",
        "            target_tokens = example.target.split()[:args.max_target_length-2]\n",
        "        target_tokens = [tokenizer.cls_token]+target_tokens+[tokenizer.sep_token]            \n",
        "        # target_ids = tokenizer.convert_tokens_to_ids(target_tokens) # MH: decoder\n",
        "        target_ids = decoder_tokenizer.convert_string_to_ids(' '.join(target_tokens))\n",
        "        target_mask = [1] *len(target_ids)\n",
        "        padding_length = args.max_target_length - len(target_ids)\n",
        "        target_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "        target_mask+=[0]*padding_length   \n",
        "\n",
        "        if example_index < 5:\n",
        "            if stage=='train':\n",
        "                print(\"*** Example ***\")\n",
        "                print(\"idx: {}\".format(example.idx))\n",
        "\n",
        "                print(\"source_tokens: {}\".format([x.replace('\\u0120','_') for x in source_tokens]))\n",
        "                print(\"source_ids: {}\".format(' '.join(map(str, source_ids))))\n",
        "                print(\"source_mask: {}\".format(' '.join(map(str, source_mask))))\n",
        "                \n",
        "                print(\"target_tokens: {}\".format([x.replace('\\u0120','_') for x in target_tokens]))\n",
        "                print(\"target_ids: {}\".format(' '.join(map(str, target_ids))))\n",
        "                print(\"target_mask: {}\".format(' '.join(map(str, target_mask))))\n",
        "       \n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                 example_index,\n",
        "                 source_ids,\n",
        "                 target_ids,\n",
        "                 source_mask,\n",
        "                 target_mask,\n",
        "            )\n",
        "        )\n",
        "    return features\n",
        "\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYHTONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    \n",
        "\n",
        "def main():\n",
        "\n",
        "    print(args)\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(args.seed)\n",
        "    # make dir if output_dir not exist\n",
        "    if os.path.exists(args.output_dir) is False:\n",
        "        os.makedirs(args.output_dir)\n",
        "        \n",
        "## model was here\n",
        "\n",
        "    if args.do_train:\n",
        "        print(\"Inside TRAIN\")\n",
        "        # Prepare training data loader\n",
        "        train_examples = read_examples(args.train_filename)\n",
        "        train_features = convert_examples_to_features(train_examples, tokenizer,args,stage='train') # MH: 2 tokenizers\n",
        "        all_source_ids = torch.tensor([f.source_ids for f in train_features], dtype=torch.long)\n",
        "        all_source_mask = torch.tensor([f.source_mask for f in train_features], dtype=torch.long)\n",
        "        all_target_ids = torch.tensor([f.target_ids for f in train_features], dtype=torch.long)\n",
        "        all_target_mask = torch.tensor([f.target_mask for f in train_features], dtype=torch.long)    \n",
        "        train_data = TensorDataset(all_source_ids,all_source_mask,all_target_ids,all_target_mask)\n",
        "        \n",
        "        if args.local_rank == -1:\n",
        "            train_sampler = RandomSampler(train_data)\n",
        "        else:\n",
        "            train_sampler = DistributedSampler(train_data)\n",
        "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size//args.gradient_accumulation_steps)\n",
        "\n",
        "        num_train_optimization_steps =  args.train_steps\n",
        "\n",
        "        # Prepare optimizer and schedule (linear warmup and decay)\n",
        "        no_decay = ['bias', 'LayerNorm.weight']\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay': args.weight_decay},\n",
        "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                    num_warmup_steps=int(t_total*0.1),\n",
        "                                                    num_training_steps=t_total)\n",
        "    \n",
        "        #Start training\n",
        "        print(\"***** Running training *****\")\n",
        "        print(\"  Num examples = %d\", len(train_examples))\n",
        "        print(\"  Batch size = %d\", args.train_batch_size)\n",
        "        print(\"  Num epoch = %d\", args.num_train_epochs)\n",
        "        \n",
        "\n",
        "        model.train()\n",
        "        dev_dataset={}\n",
        "        nb_tr_examples, nb_tr_steps,tr_loss,global_step,best_bleu,best_loss = 0, 0,0,0,0,1e6 \n",
        "        for epoch in range(args.num_train_epochs):\n",
        "            bar = tqdm(train_dataloader,total=len(train_dataloader))\n",
        "            for batch in bar:\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                source_ids,source_mask,target_ids,target_mask = batch\n",
        "                loss,_,_ = model(source_ids=source_ids,source_mask=source_mask,target_ids=target_ids,target_mask=target_mask)\n",
        "\n",
        "                if args.n_gpu > 1:\n",
        "                    loss = loss.mean() # mean() to average on multi-gpu.\n",
        "                if args.gradient_accumulation_steps > 1:\n",
        "                    loss = loss / args.gradient_accumulation_steps\n",
        "                tr_loss += loss.item()\n",
        "                train_loss=round(tr_loss*args.gradient_accumulation_steps/(nb_tr_steps+1),4)\n",
        "                bar.set_description(\"epoch {} loss {}\".format(epoch,train_loss))\n",
        "                nb_tr_examples += source_ids.size(0)\n",
        "                nb_tr_steps += 1\n",
        "                loss.backward()\n",
        "\n",
        "                if (nb_tr_steps + 1) % args.gradient_accumulation_steps == 0:\n",
        "                    #Update parameters\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "                    scheduler.step()\n",
        "                    global_step += 1\n",
        "\n",
        "            if args.do_eval:\n",
        "                print(\"Inside EVAL\")\n",
        "                #Eval model with dev dataset\n",
        "                tr_loss = 0\n",
        "                nb_tr_examples, nb_tr_steps = 0, 0                     \n",
        "                eval_flag=False    \n",
        "                if 'dev_loss' in dev_dataset:\n",
        "                    eval_examples,eval_data=dev_dataset['dev_loss']\n",
        "                else:\n",
        "                    eval_examples = read_examples(args.dev_filename)\n",
        "                    eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='dev')\n",
        "                    all_source_ids = torch.tensor([f.source_ids for f in eval_features], dtype=torch.long)\n",
        "                    all_source_mask = torch.tensor([f.source_mask for f in eval_features], dtype=torch.long)\n",
        "                    all_target_ids = torch.tensor([f.target_ids for f in eval_features], dtype=torch.long)\n",
        "                    all_target_mask = torch.tensor([f.target_mask for f in eval_features], dtype=torch.long)      \n",
        "                    eval_data = TensorDataset(all_source_ids,all_source_mask,all_target_ids,all_target_mask)   \n",
        "                    dev_dataset['dev_loss']=eval_examples,eval_data\n",
        "                eval_sampler = SequentialSampler(eval_data)\n",
        "                eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "                print(\"\\n***** Running evaluation *****\")\n",
        "                print(\"  Num examples = %d\", len(eval_examples))\n",
        "                print(\"  Batch size = %d\", args.eval_batch_size)\n",
        "\n",
        "                #Start Evaling model\n",
        "                model.eval()\n",
        "                eval_loss,tokens_num = 0,0\n",
        "                for batch in eval_dataloader:\n",
        "                    batch = tuple(t.to(device) for t in batch)\n",
        "                    source_ids,source_mask,target_ids,target_mask = batch                  \n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        _,loss,num = model(source_ids=source_ids,source_mask=source_mask,\n",
        "                                           target_ids=target_ids,target_mask=target_mask)     \n",
        "                    eval_loss += loss.sum().item()\n",
        "                    tokens_num += num.sum().item()\n",
        "                #Pring loss of dev dataset    \n",
        "                model.train()\n",
        "                eval_loss = eval_loss / tokens_num\n",
        "                result = {'eval_ppl': round(np.exp(eval_loss),5),\n",
        "                          'global_step': global_step+1,\n",
        "                          'train_loss': round(train_loss,5)}\n",
        "                for key in sorted(result.keys()):\n",
        "                    print(\"  %s = %s\", key, str(result[key]))\n",
        "                print(\"  \"+\"*\"*20)   \n",
        "\n",
        "                #save last checkpoint\n",
        "                last_output_dir = os.path.join(args.output_dir, 'checkpoint-last')\n",
        "                if not os.path.exists(last_output_dir):\n",
        "                    os.makedirs(last_output_dir)\n",
        "                model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "                output_model_file = os.path.join(last_output_dir, \"pytorch_model.bin\")\n",
        "                torch.save(model_to_save.state_dict(), output_model_file)                    \n",
        "                if eval_loss<best_loss:\n",
        "                    print(\"  Best ppl:%s\",round(np.exp(eval_loss),5))\n",
        "                    print(\"  \"+\"*\"*20)\n",
        "                    best_loss=eval_loss\n",
        "                    # Save best checkpoint for best ppl\n",
        "                    output_dir = os.path.join(args.output_dir, 'checkpoint-best-ppl')\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "                    output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
        "                    torch.save(model_to_save.state_dict(), output_model_file)  \n",
        "\n",
        "\n",
        "                #Calculate bleu  \n",
        "                print(\"Calculating BLEU\")\n",
        "                if 'dev_bleu' in dev_dataset:\n",
        "                    eval_examples,eval_data=dev_dataset['dev_bleu']\n",
        "                else:\n",
        "                    eval_examples = read_examples(args.dev_filename)\n",
        "                    eval_examples = random.sample(eval_examples,min(1000,len(eval_examples)))\n",
        "                    eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='test')\n",
        "                    all_source_ids = torch.tensor([f.source_ids for f in eval_features], dtype=torch.long)\n",
        "                    all_source_mask = torch.tensor([f.source_mask for f in eval_features], dtype=torch.long)    \n",
        "                    eval_data = TensorDataset(all_source_ids,all_source_mask)   \n",
        "                    dev_dataset['dev_bleu']=eval_examples,eval_data\n",
        "\n",
        "\n",
        "\n",
        "                eval_sampler = SequentialSampler(eval_data)\n",
        "                eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "                model.eval() \n",
        "                p=[]\n",
        "                for batch in eval_dataloader:\n",
        "                    batch = tuple(t.to(device) for t in batch)\n",
        "                    source_ids,source_mask= batch                  \n",
        "                    with torch.no_grad():\n",
        "                        preds = model(source_ids=source_ids,source_mask=source_mask)  \n",
        "                        for pred in preds:\n",
        "                            t=pred[0].cpu().numpy()\n",
        "                            t=list(t)\n",
        "                            if 0 in t:\n",
        "                                t=t[:t.index(0)]\n",
        "                            # text = tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
        "                            text = decoder_tokenizer.decode(t) # MH: decoder\n",
        "                            p.append(text)\n",
        "                model.train()\n",
        "                predictions=[]\n",
        "                with open(os.path.join(args.output_dir,\"dev.output\"),'w') as f, open(os.path.join(args.output_dir,\"dev.gold\"),'w') as f1:\n",
        "                    for ref,gold in zip(p,eval_examples):\n",
        "                        predictions.append(str(gold.idx)+'\\t'+ref)\n",
        "                        f.write(str(gold.idx)+'\\t'+ref+'\\n')\n",
        "                        f1.write(str(gold.idx)+'\\t'+gold.target+'\\n')     \n",
        "\n",
        "                (goldMap, predictionMap) = bleu.computeMaps(predictions, os.path.join(args.output_dir, \"dev.gold\")) \n",
        "                dev_bleu=round(bleu.bleuFromMaps(goldMap, predictionMap)[0],2)\n",
        "                print(\"  %s = %s \"%(\"bleu-4\",str(dev_bleu)))\n",
        "                print(\"  \"+\"*\"*20)    \n",
        "                if dev_bleu>best_bleu:\n",
        "                    print(\"  Best bleu:%s\",dev_bleu)\n",
        "                    print(\"  \"+\"*\"*20)\n",
        "                    best_bleu=dev_bleu\n",
        "                    # Save best checkpoint for best bleu\n",
        "                    output_dir = os.path.join(args.output_dir, 'checkpoint-best-bleu')\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "                    output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
        "                    torch.save(model_to_save.state_dict(), output_model_file)\n",
        "               \n",
        "    if args.do_test:\n",
        "        # print(\"Inside TEST\")\n",
        "        # files=[]\n",
        "        # if args.dev_filename is not None:\n",
        "        #     files.append(args.dev_filename)\n",
        "        # if args.test_filename is not None:\n",
        "        #     files.append(args.test_filename)\n",
        "        # for idx,file in enumerate(files):   \n",
        "        idx = 0\n",
        "        file = args.test_filename # Change it to test file later\n",
        "        print(\"Test file: {}\".format(file))\n",
        "        eval_examples = read_examples(file)\n",
        "        eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='test')\n",
        "        all_source_ids = torch.tensor([f.source_ids for f in eval_features], dtype=torch.long)\n",
        "        all_source_mask = torch.tensor([f.source_mask for f in eval_features], dtype=torch.long)    \n",
        "        eval_data = TensorDataset(all_source_ids,all_source_mask)   \n",
        "\n",
        "        # Calculate bleu\n",
        "        eval_sampler = SequentialSampler(eval_data)\n",
        "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "        model.eval() \n",
        "        p=[]\n",
        "        for batch in tqdm(eval_dataloader,total=len(eval_dataloader)):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            source_ids,source_mask= batch                  \n",
        "            with torch.no_grad():\n",
        "                preds = model(source_ids=source_ids,source_mask=source_mask) \n",
        "                for pred in preds:\n",
        "                    t=pred[0].cpu().numpy()\n",
        "                    t=list(t)\n",
        "                    if 0 in t:\n",
        "                        t=t[:t.index(0)]\n",
        "                    # text = tokenizer.decode(t,clean_up_tokenization_spaces=False) # MH: decoder\n",
        "                    text = decoder_tokenizer.decode(t)\n",
        "                    p.append(text)\n",
        "        model.train()\n",
        "        predictions=[]\n",
        "        with open(os.path.join(args.output_dir,\"test_{}.output\".format(str(idx))),'w') as f, open(os.path.join(args.output_dir,\"test_{}.gold\".format(str(idx))),'w') as f1:\n",
        "            for ref,gold in zip(p,eval_examples):\n",
        "                predictions.append(str(gold.idx)+'\\t'+ref)\n",
        "                f.write(str(gold.idx)+'\\t'+ref+'\\n')\n",
        "                f1.write(str(gold.idx)+'\\t'+gold.target+'\\n')     \n",
        "\n",
        "        (goldMap, predictionMap) = bleu.computeMaps(predictions, os.path.join(args.output_dir, args.output_name+\"test_{}.gold\".format(idx))) \n",
        "        dev_bleu=round(bleu.bleuFromMaps(goldMap, predictionMap)[0],2)\n",
        "        print(\"  %s = %s \"%(\"bleu-4\",str(dev_bleu)))\n",
        "        print(\"  \"+\"*\"*20)    \n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffH7YvV6Oaxz"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34alizypPOH9"
      },
      "source": [
        "class Arguments:\n",
        "    pass"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAaxEIuUM2uG"
      },
      "source": [
        "output_dir=\"/gdrive/My Drive/text2app_models/RoBERTa/\" # Colab + Drive\n",
        "# output_dir=\"model/\" # Local\n",
        "data_dir = '../synthesized_data/'\n",
        "train_file=data_dir+'nl_sar_train.csv'\n",
        "dev_file=data_dir+'nl_sar_valid.csv'\n",
        "test_file=data_dir+'nl_sar_test.csv'\n",
        "pretrained_model= 'roberta-base' #'microsoft/codebert-base' #'roberta-base'"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpOUBhufLt-w"
      },
      "source": [
        "args = Arguments()\n",
        "\n",
        "## Required parameters\n",
        "args.model_type='roberta'\n",
        "args.model_name_or_path=pretrained_model\n",
        "args.output_dir=output_dir\n",
        "args.load_model_path=None #output_dir+\"/checkpoint-best-bleu/pytorch_model.bin\"\n",
        "## Other parameters\n",
        "args.train_filename=train_file\n",
        "args.dev_filename=dev_file\n",
        "args.test_filename=test_file\n",
        "args.output_name = \"\"\n",
        "args.config_name=\"\"\n",
        "args.tokenizer_name=\"\"\n",
        "args.gradient_accumulation_steps=1\n",
        "args.weight_decay=0.0\n",
        "args.adam_epsilon=1e-8\n",
        "args.max_grad_norm=1.0\n",
        "args.max_steps=-1\n",
        "args.eval_steps=-1\n",
        "args.train_steps=-1\n",
        "args.warmup_steps=0\n",
        "args.local_rank=-1\n",
        "args.seed=42\n",
        "\n",
        "args.no_cuda=False\n",
        "args.do_lower_case=True\n",
        "args.do_train=True\n",
        "args.do_eval=True\n",
        "args.do_test=False\n",
        "\n",
        "args.num_train_epochs=6\n",
        "args.train_batch_size=100\n",
        "args.eval_batch_size=100\n",
        "args.learning_rate=5e-5\n",
        "args.max_source_length=80\n",
        "args.max_target_length=50\n",
        "args.beam_size=5"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSceeljfzY0I",
        "outputId": "5d540e4a-49fb-4b81-f177-5072247f08f4"
      },
      "source": [
        "# Setup CUDA, GPU & distributed training\n",
        "if args.local_rank == -1 or args.no_cuda:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "    args.n_gpu = torch.cuda.device_count()\n",
        "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "    torch.cuda.set_device(args.local_rank)\n",
        "    device = torch.device(\"cuda\", args.local_rank)\n",
        "    torch.distributed.init_process_group(backend='nccl')\n",
        "    args.n_gpu = 1\n",
        "logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s\",\n",
        "                args.local_rank, device, args.n_gpu, bool(args.local_rank != -1))\n",
        "args.device = device"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "07/01/2021 09:54:45 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YVnN-E_zEcj"
      },
      "source": [
        "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
        "config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path)\n",
        "tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,do_lower_case=args.do_lower_case)\n",
        "\n",
        "decoder_tokenizer = MyTokenizer()\n",
        "\n",
        "#budild model\n",
        "encoder = model_class.from_pretrained(args.model_name_or_path,config=config)    \n",
        "decoder_layer = nn.TransformerDecoderLayer(d_model=config.hidden_size, nhead=config.num_attention_heads)\n",
        "decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_SNN0D9EB8p"
      },
      "source": [
        "model=Seq2Seq(encoder=encoder,decoder=decoder,config=config,\n",
        "              beam_size=args.beam_size,max_length=args.max_target_length,\n",
        "              sos_id=tokenizer.cls_token_id,eos_id=tokenizer.sep_token_id)\n",
        "if args.load_model_path is not None:\n",
        "    print(\"reload model from {}\".format(args.load_model_path))\n",
        "    model.load_state_dict(torch.load(args.load_model_path))\n",
        "    \n",
        "model.to(device)\n",
        "if args.local_rank != -1:\n",
        "    # Distributed training\n",
        "    try:\n",
        "        from apex.parallel import DistributedDataParallel as DDP\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
        "    model = DDP(model)\n",
        "elif args.n_gpu > 1:\n",
        "    # multi-gpu training\n",
        "    model = torch.nn.DataParallel(model)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8fS_vh0I0P5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d773b294-d1fc-44da-8941-3ca090ee0960"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.Arguments object at 0x7f8376e41690>\n",
            "Inside TRAIN\n",
            "*** Example ***\n",
            "idx: 0\n",
            "source_tokens: ['<s>', 'create', '_a', '_mobile', '_app', '_with', '_a', '_text', '2', 'speech', '_,', '_a', '_button', '_for', '_a', '_label', '_,', '_a', '_text', 'box', '_,', '_a', '_switch', '_,', '_optional', '_password', 'text', 'box', '_,', '_an', '_audio', '_with', '_source', '_string', '0', '_,', '_and', '_a', '_time', '_pick', 'er', '_.', '_if', '_the', '_button', '_is', '_touched', ',', '_set', '_the', '_label', '_adjacent', '_to', '_the', '_text', '_in', '_the', '_box', '_.', '_if', '_the', '_switch', '_is', '_pressed', ',', '_restart', '_the', '_player', '_.', '</s>']\n",
            "source_ids: 0 32845 10 1830 1553 19 10 2788 176 40511 2156 10 6148 13 10 6929 2156 10 2788 8304 2156 10 5405 2156 17679 14844 29015 8304 2156 41 6086 19 1300 6755 288 2156 8 10 86 1339 254 479 114 5 6148 16 6699 6 278 5 6929 12142 7 5 2788 11 5 2233 479 114 5 5405 16 11224 6 12721 5 869 479 2 1 1 1 1 1 1 1 1 1 1\n",
            "source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
            "target_tokens: ['<s>', '<complist>', '<text2speech>', '<button>', 'button1', '</button>', '<label>', 'label1', '</label>', '<textbox>', '<switch>', 'switch1', '</switch>', '<passwordtextbox>', '<player>', 'string0', '</player>', '<timepicker>', '</complist>', '<code>', '<button1clicked>', '<label1>', '<textboxtext1>', '</label>', '</button1clicked>', '<switch1flipped>', '<player1>', '<stop>', '</player>', '</switch1flipped>', '</code>', '</s>']\n",
            "target_ids: 0 50 82 44 100 13 59 105 19 83 80 123 27 64 70 113 20 93 17 48 39 56 84 19 8 76 66 75 20 23 15 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "*** Example ***\n",
            "idx: 1\n",
            "source_tokens: ['<s>', 'save', '_me', '_!', '_mobile', '_app', '_includes', '_a', '_text', 'box', '_,', '_a', '_video', '_with', '_source', '_string', '0', '_,', '_and', '_a', '_circle', '_.', '_if', '_the', '_circle', '_is', '_touches', '_edge', ',', '_stop', '_video', '_.', '</s>']\n",
            "source_ids: 0 31575 162 27785 1830 1553 1171 10 2788 8304 2156 10 569 19 1300 6755 288 2156 8 10 7922 479 114 5 7922 16 12325 3543 6 912 569 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "target_tokens: ['<s>', '<complist>', '<textbox>', '<video_player>', 'string0', '</video_player>', '<ball>', '</complist>', '<code>', '<ball1reach_edge>', '<video_player1>', '<stop>', '</video_player>', '</ball1reach_edge>', '</code>', '</s>']\n",
            "target_ids: 0 50 83 98 113 29 35 17 48 34 94 75 29 7 15 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "*** Example ***\n",
            "idx: 2\n",
            "source_tokens: ['<s>', 'make', '_mobile', '_application', '_that', '_requires', '_a', '_camera', '_,', '_a', '_text', '_box', '_,', '_another', '_text', '2', 'speech', '_,', '_a', '_video', '_box', '_with', '_source', '_string', '0', '_,', '_a', '_circle', '_,', '_a', '_motion', '_sensor', '_,', '_a', '_pas', 'word', '_text', '_box', '_,', '_a', '_label', '_,', '_a', '_video', '_,', '_a', '_random', '_clip', '_,', '_a', '_switch', '_,', '_and', '_a', '_video', '_with', '_source', '_string', '1', '_.', '_when', '_the', '_circle', '_is', '_touches', '_please', '_tell', '_the', '_text', '_in', '_the', '_text', 'box', '_.', '_if', '_the', '_acceler', 'ometer', '_is', '</s>']\n",
            "source_ids: 0 19746 1830 2502 14 3441 10 2280 2156 10 2788 2233 2156 277 2788 176 40511 2156 10 569 2233 19 1300 6755 288 2156 10 7922 2156 10 4298 9626 2156 10 6977 14742 2788 2233 2156 10 6929 2156 10 569 2156 10 9624 7200 2156 10 5405 2156 8 10 569 19 1300 6755 134 479 77 5 7922 16 12325 2540 1137 5 2788 11 5 2788 8304 479 114 5 27416 12687 16 2\n",
            "source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "target_tokens: ['<s>', '<complist>', '<camera>', '<textbox>', '<text2speech>', '<video_player>', 'string0', '</video_player>', '<ball>', '<accelerometer>', '<passwordtextbox>', '<label>', 'label1', '</label>', '<video_player>', 'random_video_player_source', '</video_player>', '<switch>', 'switch1', '</switch>', '<video_player>', 'string1', '</video_player>', '</complist>', '<code>', '<ball1flung>', '<text2speech1>', '<textboxtext1>', '</text2speech1>', '</ball1flung>', '<accelerometer1shaken>', '<video_player1>', '<start>', '</video_player>', '</accelerometer1shaken>', '<switch1flipped>', '<video_player2>', '<start>', '</video_player>', '</switch1flipped>', '</code>', '</s>']\n",
            "target_ids: 0 50 46 83 82 98 113 29 35 31 64 59 105 19 98 112 29 80 123 27 98 114 29 17 48 33 81 84 28 6 30 94 74 29 4 76 95 74 29 23 15 2 1 1 1 1 1 1 1 1\n",
            "target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
            "*** Example ***\n",
            "idx: 3\n",
            "source_tokens: ['<s>', 'mobile', '_app', '_containing', '_a', '_movement', '_sensor', '_,', '_a', '_microphone', '_,', '_and', '_a', '_speech', '_.', '_when', '_the', '_shake', '_knob', '_is', '_shaken', ',', '_speak', '_the', '_text', 'box', '_content', '_.', '</s>']\n",
            "source_ids: 0 25254 1553 8200 10 2079 9626 2156 10 18896 2156 8 10 1901 479 77 5 8559 42256 16 17548 6 1994 5 2788 8304 1383 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "target_tokens: ['<s>', '<complist>', '<accelerometer>', '<textbox>', '<text2speech>', '</complist>', '<code>', '<accelerometer1shaken>', '<text2speech1>', '<textboxtext1>', '</text2speech1>', '</accelerometer1shaken>', '</code>', '</s>']\n",
            "target_ids: 0 50 31 83 82 17 48 30 81 84 28 4 15 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "*** Example ***\n",
            "idx: 4\n",
            "source_tokens: ['<s>', 'create', '_a', '_mobile', '_app', '_containing', '_a', '_pas', 'word', '_text', '_box', '_;', '_a', '_string', '_,', '_a', '_label', '_,', '_a', '_button', '_called', '_string', '0', '_,', '_a', '_label', '_named', '_string', '1', '_,', '_a', '_button', '_,', '_a', '_ball', '_,', '_and', '_a', '_time', '_pick', 'er', '_.', '_whenever', '_the', '_switch', '_is', '_clicked', ',', '_set', '_label', '_text', '_to', '_the', '_time', '_.', '_if', '_the', '_initial', '_button', '_is', '_clicked', ',', '_set', '_label', '_string', '2', '_.', '_when', '_clicked', '_the', '_later', '_button', '_is', '_flipped', '_set', '_the', '_speed', '_to', '_number', '</s>']\n",
            "source_ids: 0 32845 10 1830 1553 8200 10 6977 14742 2788 2233 25606 10 6755 2156 10 6929 2156 10 6148 373 6755 288 2156 10 6929 1440 6755 134 2156 10 6148 2156 10 1011 2156 8 10 86 1339 254 479 8378 5 5405 16 28551 6 278 6929 2788 7 5 86 479 114 5 2557 6148 16 28551 6 278 6929 6755 176 479 77 28551 5 423 6148 16 18626 278 5 2078 7 346 2\n",
            "source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "target_tokens: ['<s>', '<complist>', '<passwordtextbox>', '<switch>', 'switch1', '</switch>', '<label>', 'label1', '</label>', '<button>', 'string0', '</button>', '<label>', 'string1', '</label>', '<button>', 'button2', '</button>', '<ball>', '<timepicker>', '</complist>', '<code>', '<switch1flipped>', '<label1>', '<time1>', '</label>', '</switch1flipped>', '<button1clicked>', '<label2>', 'string2', '</label>', '</button1clicked>', '<button2clicked>', '<ball1>', '<speed>', 'number0', '</speed>', '</ball1>', '</button2clicked>', '</code>', '</s>']\n",
            "target_ids: 0 50 64 80 123 27 59 105 19 44 113 13 59 114 19 44 101 13 35 93 17 48 76 56 88 19 23 39 57 115 19 8 40 32 73 110 22 5 9 15 2 1 1 1 1 1 1 1 1 1\n",
            "target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/755 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = %d 75449\n",
            "  Batch size = %d 100\n",
            "  Num epoch = %d 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0 loss 4.8811:   0%|          | 1/755 [00:02<26:03,  2.07s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-292a2fdff8de>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0msource_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msource_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msource_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-325bf4da858a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, source_ids, source_mask, target_ids, target_mask, args)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1e4\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtarget_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtarget_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mtgt_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## MH: Problmatic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msource_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    246\u001b[0m                          \u001b[0mmemory_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                          \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m                          memory_key_padding_mask=memory_key_padding_mask)\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n\u001b[0;32m--> 409\u001b[0;31m                                    key_padding_mask=memory_key_padding_mask)[0]\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m   1036\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m                 attn_mask=attn_mask)\n\u001b[0m\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   5080\u001b[0m     \u001b[0;31m# (deep breath) calculate attention and out projection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5081\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5082\u001b[0;31m     \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_scaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5083\u001b[0m     \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5084\u001b[0m     \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_scaled_dot_product_attention\u001b[0;34m(q, k, v, attn_mask, dropout_p)\u001b[0m\n\u001b[1;32m   4826\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mattn_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4827\u001b[0m         \u001b[0mattn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4828\u001b[0;31m     \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4829\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdropout_p\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4830\u001b[0m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1677\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1679\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1680\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.76 GiB total capacity; 13.31 GiB already allocated; 17.75 MiB free; 13.68 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TptSjxB5Ye96"
      },
      "source": [
        "# If working on colab, save checkpoint to Google Drive\n",
        "# !cp model/checkpoint-best-bleu/pytorch_model.bin '/gdrive/My Drive/text2app_models/RoBERTa/' # CodeBERT RoBERTa PointerNet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIV6rQ0oOUPd"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-V6r9YtBOPWG"
      },
      "source": [
        "args.do_test=True\n",
        "args.do_train=False\n",
        "args.beam_size=1\n",
        "args.test_filename=data_dir+'nl_sar_test_unseen_pair.csv'\n",
        "args.output_name = \"unseen_pair_\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LDVxq3eJ4Xf"
      },
      "source": [
        "**Test Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5Do7psBVMLW"
      },
      "source": [
        "model=Seq2Seq(encoder=encoder,decoder=decoder,config=config,\n",
        "              beam_size=args.beam_size,max_length=args.max_target_length,\n",
        "              sos_id=tokenizer.cls_token_id,eos_id=tokenizer.sep_token_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUa9ag27Dyms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b81091c5-73dd-4f97-eec7-fc5cf6a1c859"
      },
      "source": [
        "# args.load_model_path=output_dir+\"/checkpoint-best-bleu/pytorch_model.bin\"\n",
        "args.load_model_path='/gdrive/My Drive/text2app_models/RoBERTa/pytorch_model.bin'\n",
        "\n",
        "model.load_state_dict(torch.load(args.load_model_path))\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (decoder): TransformerDecoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (3): TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (4): TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (5): TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (lm_head): Linear(in_features=768, out_features=115, bias=False)\n",
              "  (lsm): LogSoftmax(dim=-1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x15hh5QgBweO"
      },
      "source": [
        "test_files = ['nl_sar_test.csv', 'unseen_test.csv']\n",
        "              # 'nl_sar_test_2%_mutation.csv', 'nl_sar_test_5%_mutation.csv', \n",
        "              #     'nl_sar_test_10%_mutation.csv', 'nl_sar_test_unseen_pair.csv']\n",
        "              \n",
        "for test_file in test_files:\n",
        "  args.test_filename=data_dir+test_file\n",
        "  args.output_name=test_file[10:-4]\n",
        "  print(\"#### testing\", test_file)\n",
        "\n",
        "  if __name__ == \"__main__\":\n",
        "      main()\n",
        "\n",
        "  ref = open(\"model/test_0.gold\")\n",
        "  ref_sar = ref.readlines()\n",
        "  ref.close()\n",
        "  pred = open(\"model/\"+args.output_name+\"test_0.output\")\n",
        "  pred_sar = pred.readlines()\n",
        "  pred.close()\n",
        "  correct = 0\n",
        "  for i in range(len(ref_sar)):\n",
        "    if ref_sar[i]==pred_sar[i]:\n",
        "      correct+=1\n",
        "  print(\"Exact Match: \", 100*correct/len(ref_sar))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVNsWW7HQNvs"
      },
      "source": [
        "## Calculate BLEU-4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNOFrVjDQQbr",
        "outputId": "d0edbe83-a3e1-4d0b-84ed-04cf0d400629"
      },
      "source": [
        "!python evaluator.py model/test_0.gold < model/test_0.output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total: 5000\n",
            "97.20048209170629\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfVbwSg4bnpx"
      },
      "source": [
        "## Exact Match"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paPK1mbrRtmF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2678aca8-8ec5-4d29-834f-cec7e3cafd8b"
      },
      "source": [
        "ref = open(\"model/test_0.gold\")\n",
        "ref_sar = ref.readlines()\n",
        "ref.close()\n",
        "\n",
        "pred = open(\"model/test_0.output\")\n",
        "pred_sar = pred.readlines()\n",
        "pred.close()\n",
        "\n",
        "correct = 0\n",
        "for i in range(len(ref_sar)):\n",
        "  if ref_sar[i]==pred_sar[i]:\n",
        "    correct+=1\n",
        "\n",
        "100*correct/len(ref_sar)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "77.8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l_2F79ey3se"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeHTIMPkQ3Y4"
      },
      "source": [
        "## Single NL prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfpi_W0EdNvZ"
      },
      "source": [
        "def single_example_to_feature(example, tokenizer): # MH: make it encoder_tokenizer, decoder_tokenizer\n",
        "    features = []\n",
        "    source_tokens = tokenizer.tokenize(example)[:args.max_source_length-2]\n",
        "    source_tokens =[tokenizer.cls_token]+source_tokens+[tokenizer.sep_token]\n",
        "    source_ids =  tokenizer.convert_tokens_to_ids(source_tokens) \n",
        "    source_mask = [1] * (len(source_tokens))\n",
        "    padding_length = args.max_source_length - len(source_ids)\n",
        "    source_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "    source_mask+=[0]*padding_length\n",
        " \n",
        "    target_tokens = ['None']\n",
        "    target_tokens = [tokenizer.cls_token]+target_tokens+[tokenizer.sep_token]            \n",
        "    # target_ids = tokenizer.convert_tokens_to_ids(target_tokens) # MH: decoder\n",
        "    target_ids = decoder_tokenizer.convert_string_to_ids(' '.join(target_tokens))\n",
        "    target_mask = [1] *len(target_ids)\n",
        "    padding_length = args.max_target_length - len(target_ids)\n",
        "    target_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "    target_mask+=[0]*padding_length   \n",
        "\n",
        "    features.append(\n",
        "        InputFeatures(\n",
        "              0,\n",
        "              source_ids,\n",
        "              target_ids,\n",
        "              source_mask,\n",
        "              target_mask,\n",
        "        )\n",
        "    )\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJPiYhLiIWgK"
      },
      "source": [
        "args.eval_batch_size = 1\n",
        "args.do_test=True\n",
        "args.do_train=False\n",
        "args.beam_size=1\n",
        "args.load_model_path=output_dir+\"/checkpoint-best-bleu/pytorch_model.bin\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogRkbfcwfDph"
      },
      "source": [
        "model.eval() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldJhnZ92w3aP"
      },
      "source": [
        "def get_sar(eval_example):\n",
        "  eval_features = single_example_to_feature(eval_example, tokenizer)\n",
        "  all_source_ids = torch.tensor([f.source_ids for f in eval_features], dtype=torch.long).to(device)\n",
        "  all_source_mask = torch.tensor([f.source_mask for f in eval_features], dtype=torch.long).to(device)\n",
        "  preds = model(source_ids=all_source_ids, source_mask=all_source_mask)  \n",
        "  pred_list = list(preds[0][0].cpu().numpy())\n",
        "  predicted_text = decoder_tokenizer.decode(pred_list[:pred_list.index(0)])\n",
        "  return predicted_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l40SKW1237Wr"
      },
      "source": [
        "eval_example = 'make an app with a textbox, a button named \"tweet\", and a label. When the button is pressed, set the label to textbox text..'\n",
        "\n",
        "get_sar(eval_example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6iq_DHvHBqJ"
      },
      "source": [
        "\"string0\": go\n",
        "\"string1\": back\n",
        "<complist> <textbox> <button> go </button> <button> back </button> </complist> <code> <button1clicked> <label1> <textboxtext1> </label1> </button1clicked> <button2clicked> <ball1> <color> <gray> </color> </ball1> </button2clicked> </code>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3EpBuayf0dp"
      },
      "source": [
        "get_sar(\"make an app with an accelerometer and a music player . when accelerometer is shaken play music\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKCkz3n0gFj3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}