{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RoBERTa to SAR",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "292024ba0d0945d692ff6b0cb0f83df4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a3eda0b179ba421ca951dde0d74e07a4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c9809e49e903489b8f89cd5706ccd9f4",
              "IPY_MODEL_62153797884e4062b66b0ce321a838d3"
            ]
          }
        },
        "a3eda0b179ba421ca951dde0d74e07a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c9809e49e903489b8f89cd5706ccd9f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_55cb5c86b1d24ed789001115f6d38b67",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 481,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 481,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_78d92543fe054c11a364371b768ec31b"
          }
        },
        "62153797884e4062b66b0ce321a838d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_13cba081479c435d9a84d2d22171fd5e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 481/481 [01:32&lt;00:00, 5.21B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cd9e7c6805534330b7d6bea55428a0c9"
          }
        },
        "55cb5c86b1d24ed789001115f6d38b67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "78d92543fe054c11a364371b768ec31b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "13cba081479c435d9a84d2d22171fd5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cd9e7c6805534330b7d6bea55428a0c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "93d52f92c87442a5a1c9e31a2479a5c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b22a9ab3e86546b1a1c543563e2a9257",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_91811922ac8b4002b415c823d4760ed9",
              "IPY_MODEL_433059e4d86e4475ad1727a97528eef2"
            ]
          }
        },
        "b22a9ab3e86546b1a1c543563e2a9257": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "91811922ac8b4002b415c823d4760ed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7bc22a9f81944aebb211cb35df287248",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898823,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898823,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4f79e77351d64b9491336234c50b5ef3"
          }
        },
        "433059e4d86e4475ad1727a97528eef2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bdf166241e81437497e683493650c8c8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:00&lt;00:00, 1.10MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bc1b819316764aa59f3ebe14cfadd716"
          }
        },
        "7bc22a9f81944aebb211cb35df287248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4f79e77351d64b9491336234c50b5ef3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bdf166241e81437497e683493650c8c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bc1b819316764aa59f3ebe14cfadd716": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "344264c011a346e8965e87d5d902e444": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2a51d81c2bee437b838f78337d0cb8d7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_eb911ccbe057437f9181d9422b0ce02e",
              "IPY_MODEL_27e52256b8a943e590f24e34497c2a12"
            ]
          }
        },
        "2a51d81c2bee437b838f78337d0cb8d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eb911ccbe057437f9181d9422b0ce02e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_063851eec58a47e0bacdfab19af71dc0",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_62f669d89b0646e094110ce4fcc3bc21"
          }
        },
        "27e52256b8a943e590f24e34497c2a12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b99e680b5376497e905104a9f4176ffb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:02&lt;00:00, 197kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dcde829fe15f48dbb71855ef1106e8f9"
          }
        },
        "063851eec58a47e0bacdfab19af71dc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "62f669d89b0646e094110ce4fcc3bc21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b99e680b5376497e905104a9f4176ffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dcde829fe15f48dbb71855ef1106e8f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dadca3763a1e456daea6f92aa4cff30b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_54d687ad53324815a870235b5e339602",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cf4193b56a8042fa8ba276e8b3eb9b0f",
              "IPY_MODEL_9d79a27651a44658a98435690b8d4c45"
            ]
          }
        },
        "54d687ad53324815a870235b5e339602": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cf4193b56a8042fa8ba276e8b3eb9b0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3f7b4f0877ad4f50aedfbc9f0ee44088",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1355863,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1355863,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_33cd2e5f093945d58864252556fb3ff7"
          }
        },
        "9d79a27651a44658a98435690b8d4c45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9c2858131bf649b08346347d23fb2756",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 4.15MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d8c433dbce624f6fa74b35eabb187f2f"
          }
        },
        "3f7b4f0877ad4f50aedfbc9f0ee44088": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "33cd2e5f093945d58864252556fb3ff7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9c2858131bf649b08346347d23fb2756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d8c433dbce624f6fa74b35eabb187f2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "182d806726524994a06ee960b78a790b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7a2eeb308d234ca0bf0c0f55f5a3c82b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_155c7edb8bbf43649f58de02dfbe1ce3",
              "IPY_MODEL_b644a64c67c446409ea33015754a0d4e"
            ]
          }
        },
        "7a2eeb308d234ca0bf0c0f55f5a3c82b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "155c7edb8bbf43649f58de02dfbe1ce3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1dcccd50354b40e48571ad0b491681a2",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 501200538,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 501200538,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c5630e809a2148c58af29d3834dd63fb"
          }
        },
        "b644a64c67c446409ea33015754a0d4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_db49c8b057a44332837b3a03b7ddabe8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 501M/501M [01:16&lt;00:00, 6.56MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d6d9118742fe4b84a7646d7450e713ae"
          }
        },
        "1dcccd50354b40e48571ad0b491681a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c5630e809a2148c58af29d3834dd63fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "db49c8b057a44332837b3a03b7ddabe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d6d9118742fe4b84a7646d7450e713ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Masum06/Text2App/blob/master/notebooks/RoBERTa_to_SAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F864UIM1YUEq"
      },
      "source": [
        "Code borrowed from: https://github.com/microsoft/CodeXGLUE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nc12sfRZSmxT",
        "outputId": "61bdb49c-3b66-4564-9602-2078730f19d9"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jul  1 17:02:38 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBG3u_eYQHKY",
        "outputId": "606d8cf4-a76f-4f2f-fae0-f789d1ba5176"
      },
      "source": [
        "!pip install transformers==4.5.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/91/61d69d58a1af1bd81d9ca9d62c90a6de3ab80d77f27c5df65d9a2c1f5626/transformers-4.5.0-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 8.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 26.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (4.41.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 45.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.0) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (2021.5.30)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.0) (2.4.7)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHNqpkh3Q2MG",
        "outputId": "a701c333-4d09-4603-ad45-c273de6b757d"
      },
      "source": [
        "!git clone https://github.com/Masum06/Text2App.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Text2App'...\n",
            "remote: Enumerating objects: 578, done.\u001b[K\n",
            "remote: Counting objects: 100% (181/181), done.\u001b[K\n",
            "remote: Compressing objects: 100% (141/141), done.\u001b[K\n",
            "remote: Total 578 (delta 117), reused 73 (delta 39), pack-reused 397\u001b[K\n",
            "Receiving objects: 100% (578/578), 241.81 MiB | 37.29 MiB/s, done.\n",
            "Resolving deltas: 100% (199/199), done.\n",
            "Checking out files: 100% (227/227), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRLNH4uJG7rI"
      },
      "source": [
        "# Text2App"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jd61C2XOiyd"
      },
      "source": [
        "Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "v6ziTZ3aTflj",
        "outputId": "c0cdca81-ade5-4a39-bfd6-e6dc193a88bc"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG59i7w5RCu_",
        "outputId": "efda8bf2-e159-4869-c620-1c9fbb6f4f7b"
      },
      "source": [
        "cd Text2App/training_RoBERTa/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Text2App/training_RoBERTa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx5HeX1IZK17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc40e0f6-56ac-4519-8e77-21f489df0037"
      },
      "source": [
        "# For saving checkpoint to Google Drive while working on Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yud9saHSVRi"
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv('../synthesized_data/nl_sar_train.csv')\n",
        "dev = pd.read_csv('../synthesized_data/nl_sar_valid.csv')\n",
        "test = pd.read_csv('../synthesized_data/nl_sar_test.csv')\n",
        "unseen_test = pd.read_csv('../synthesized_data/unseen_test.csv')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0AwB1I6Yhle"
      },
      "source": [
        "class MyTokenizer:\n",
        "  vocab_size = 0\n",
        "  vocab = []\n",
        "  id_to_token = {}\n",
        "  token_to_id = {}\n",
        "\n",
        "  def __init__(self):\n",
        "    self.vocab = list(set(\" \".join(list(train['SAR'])+list(dev['SAR'])+list(test['SAR'])).split()))\n",
        "    self.vocab.sort()\n",
        "    self.vocab_size = len(self.token_to_id) \n",
        "    # Special tokens: <s><pad></s><unk>\n",
        "    self.add_token('<s>')\n",
        "    self.add_token('<pad>')\n",
        "    self.add_token('</s>')\n",
        "    self.add_token('<unk>')\n",
        "    for v in self.vocab:\n",
        "      self.add_token(v)\n",
        "    self.add_token('None')\n",
        "    \n",
        "    infile = open('roberta_decoder.vocab','rb')\n",
        "    prev_vocab = pickle.load(infile)\n",
        "    if prev_vocab != self.vocab:\n",
        "      outfile = open(\"roberta_decoder.vocab\",'wb')\n",
        "      pickle.dump(self.vocab, outfile)\n",
        "      print(\"New vocab file created! Update training_RoBERTa/roberta_decoder.vocab.\")\n",
        "\n",
        "  def tokenize(self, s):\n",
        "    return s.split()\n",
        "\n",
        "  def add_token(self, s):\n",
        "    if s not in self.token_to_id:\n",
        "      self.id_to_token[self.vocab_size] = s\n",
        "      self.token_to_id[s] = self.vocab_size\n",
        "      self.vocab_size+=1\n",
        "\n",
        "  def convert_string_to_ids(self, s):\n",
        "    tokens = s.split()\n",
        "    ids = []\n",
        "    for token in tokens:\n",
        "      ids.append(self.token_to_id[token])\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \"\"\n",
        "    for id in ids:\n",
        "      text += self.id_to_token[id] + \" \"\n",
        "    return text[:-1]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLOsuOtfPByQ"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VbExmvP0-Aw"
      },
      "source": [
        "# Copyright (c) Microsoft Corporation. \n",
        "# Licensed under the MIT license.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import copy\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "        Build Seqence-to-Sequence.\n",
        "        \n",
        "        Parameters:\n",
        "        * `encoder`- encoder of seq2seq model. e.g. roberta\n",
        "        * `decoder`- decoder of seq2seq model. e.g. transformer\n",
        "        * `config`- configuration of encoder model. \n",
        "        * `beam_size`- beam size for beam search. \n",
        "        * `max_length`- max length of target for beam search. \n",
        "        * `sos_id`- start of symbol ids in target for beam search.\n",
        "        * `eos_id`- end of symbol ids in target for beam search. \n",
        "    \"\"\"\n",
        "    def __init__(self, encoder,decoder,config,beam_size=None,max_length=None,sos_id=None,eos_id=None):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder=decoder\n",
        "        self.config=config\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(2048, 2048)))\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.lm_head = nn.Linear(config.hidden_size, decoder_tokenizer.vocab_size, bias=False) #config.vocab_size\n",
        "        self.lsm = nn.LogSoftmax(dim=-1)\n",
        "        # self.tie_weights()\n",
        "        \n",
        "        self.beam_size=beam_size\n",
        "        self.max_length=max_length\n",
        "        self.sos_id=sos_id\n",
        "        self.eos_id=eos_id\n",
        "        \n",
        "    def _tie_or_clone_weights(self, first_module, second_module):\n",
        "        \"\"\" Tie or clone module weights depending of weither we are using TorchScript or not\n",
        "        \"\"\"\n",
        "        if self.config.torchscript:\n",
        "            first_module.weight = nn.Parameter(second_module.weight.clone())\n",
        "        else:\n",
        "            first_module.weight = second_module.weight\n",
        "                  \n",
        "    def tie_weights(self):\n",
        "        \"\"\" Make sure we are sharing the input and output embeddings.\n",
        "            Export to TorchScript can't handle parameter sharing so we are cloning them instead.\n",
        "        \"\"\"\n",
        "        self._tie_or_clone_weights(self.lm_head,\n",
        "                                   self.encoder.embeddings.word_embeddings)        \n",
        "        \n",
        "    def forward(self, source_ids=None,source_mask=None,target_ids=None,target_mask=None,args=None):   \n",
        "        outputs = self.encoder(source_ids, attention_mask=source_mask)\n",
        "        encoder_output = outputs[0].permute([1,0,2]).contiguous()\n",
        "        if target_ids is not None:  \n",
        "            attn_mask=-1e4 *(1-self.bias[:target_ids.shape[1],:target_ids.shape[1]])\n",
        "            tgt_embeddings = self.encoder.embeddings(target_ids).permute([1,0,2]).contiguous() ## MH: Problmatic\n",
        "            out = self.decoder(tgt_embeddings,encoder_output,tgt_mask=attn_mask,memory_key_padding_mask=(1-source_mask).bool())\n",
        "            hidden_states = torch.tanh(self.dense(out)).permute([1,0,2]).contiguous()\n",
        "            lm_logits = self.lm_head(hidden_states)\n",
        "            # Shift so that tokens < n predict n\n",
        "            active_loss = target_mask[..., 1:].ne(0).view(-1) == 1\n",
        "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
        "            shift_labels = target_ids[..., 1:].contiguous()\n",
        "            # Flatten the tokens\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1))[active_loss],\n",
        "                            shift_labels.view(-1)[active_loss])\n",
        "\n",
        "            outputs = loss, loss*active_loss.sum(), active_loss.sum()\n",
        "            # print(\"Inside forward, outputs:\", outputs)\n",
        "            return outputs\n",
        "        else:\n",
        "            #Predict \n",
        "            preds=[]       \n",
        "            zero=torch.cuda.LongTensor(1).fill_(0)     \n",
        "            for i in range(source_ids.shape[0]):\n",
        "                context=encoder_output[:,i:i+1]\n",
        "                context_mask=source_mask[i:i+1,:]\n",
        "                beam = Beam(self.beam_size,self.sos_id,self.eos_id)\n",
        "                input_ids=beam.getCurrentState()\n",
        "                context=context.repeat(1, self.beam_size,1)\n",
        "                context_mask=context_mask.repeat(self.beam_size,1)\n",
        "                for _ in range(self.max_length): \n",
        "                    if beam.done():\n",
        "                        break\n",
        "                    attn_mask=-1e4 *(1-self.bias[:input_ids.shape[1],:input_ids.shape[1]])\n",
        "                    tgt_embeddings = self.encoder.embeddings(input_ids).permute([1,0,2]).contiguous()\n",
        "                    out = self.decoder(tgt_embeddings,context,tgt_mask=attn_mask,memory_key_padding_mask=(1-context_mask).bool())\n",
        "                    out = torch.tanh(self.dense(out))\n",
        "                    hidden_states=out.permute([1,0,2]).contiguous()[:,-1,:]\n",
        "                    out = self.lsm(self.lm_head(hidden_states)).data\n",
        "                    beam.advance(out)\n",
        "                    input_ids.data.copy_(input_ids.data.index_select(0, beam.getCurrentOrigin()))\n",
        "                    input_ids=torch.cat((input_ids,beam.getCurrentState()),-1)\n",
        "                hyp= beam.getHyp(beam.getFinal())\n",
        "                pred=beam.buildTargetTokens(hyp)[:self.beam_size]\n",
        "                pred=[torch.cat([x.view(-1) for x in p]+[zero]*(self.max_length-len(p))).view(1,-1) for p in pred]\n",
        "                preds.append(torch.cat(pred,0).unsqueeze(0))\n",
        "                \n",
        "            preds=torch.cat(preds,0)\n",
        "            # print(\"Inside Forward, preds: \", preds)\n",
        "            return preds   \n",
        "        \n",
        "        \n",
        "\n",
        "class Beam(object):\n",
        "    def __init__(self, size,sos,eos):\n",
        "        self.size = size\n",
        "        self.tt = torch.cuda\n",
        "        # The score for each translation on the beam.\n",
        "        self.scores = self.tt.FloatTensor(size).zero_()\n",
        "        # The backpointers at each time-step.\n",
        "        self.prevKs = []\n",
        "        # The outputs at each time-step.\n",
        "        self.nextYs = [self.tt.LongTensor(size)\n",
        "                       .fill_(0)]\n",
        "        self.nextYs[0][0] = sos\n",
        "        # Has EOS topped the beam yet.\n",
        "        self._eos = eos\n",
        "        self.eosTop = False\n",
        "        # Time and k pair for finished.\n",
        "        self.finished = []\n",
        "\n",
        "    def getCurrentState(self):\n",
        "        \"Get the outputs for the current timestep.\"\n",
        "        batch = self.tt.LongTensor(self.nextYs[-1]).view(-1, 1)\n",
        "        return batch\n",
        "\n",
        "    def getCurrentOrigin(self):\n",
        "        \"Get the backpointers for the current timestep.\"\n",
        "        return self.prevKs[-1]\n",
        "\n",
        "    def advance(self, wordLk):\n",
        "        \"\"\"\n",
        "        Given prob over words for every last beam `wordLk` and attention\n",
        "        `attnOut`: Compute and update the beam search.\n",
        "        Parameters:\n",
        "        * `wordLk`- probs of advancing from the last step (K x words)\n",
        "        * `attnOut`- attention at the last step\n",
        "        Returns: True if beam search is complete.\n",
        "        \"\"\"\n",
        "        numWords = wordLk.size(1)\n",
        "\n",
        "        # Sum the previous scores.\n",
        "        if len(self.prevKs) > 0:\n",
        "            beamLk = wordLk + self.scores.unsqueeze(1).expand_as(wordLk)\n",
        "\n",
        "            # Don't let EOS have children.\n",
        "            for i in range(self.nextYs[-1].size(0)):\n",
        "                if self.nextYs[-1][i] == self._eos:\n",
        "                    beamLk[i] = -1e20\n",
        "        else:\n",
        "            beamLk = wordLk[0]\n",
        "        flatBeamLk = beamLk.view(-1)\n",
        "        bestScores, bestScoresId = flatBeamLk.topk(self.size, 0, True, True)\n",
        "\n",
        "        self.scores = bestScores\n",
        "\n",
        "        # bestScoresId is flattened beam x word array, so calculate which\n",
        "        # word and beam each score came from\n",
        "        prevK = bestScoresId // numWords\n",
        "        self.prevKs.append(prevK)\n",
        "        self.nextYs.append((bestScoresId - prevK * numWords))\n",
        "\n",
        "\n",
        "        for i in range(self.nextYs[-1].size(0)):\n",
        "            if self.nextYs[-1][i] == self._eos:\n",
        "                s = self.scores[i]\n",
        "                self.finished.append((s, len(self.nextYs) - 1, i))\n",
        "\n",
        "        # End condition is when top-of-beam is EOS and no global score.\n",
        "        if self.nextYs[-1][0] == self._eos:\n",
        "            self.eosTop = True\n",
        "\n",
        "    def done(self):\n",
        "        return self.eosTop and len(self.finished) >=self.size\n",
        "\n",
        "    def getFinal(self):\n",
        "        if len(self.finished) == 0:\n",
        "            self.finished.append((self.scores[0], len(self.nextYs) - 1, 0))\n",
        "        self.finished.sort(key=lambda a: -a[0])\n",
        "        if len(self.finished) != self.size:\n",
        "            unfinished=[]\n",
        "            for i in range(self.nextYs[-1].size(0)):\n",
        "                if self.nextYs[-1][i] != self._eos:\n",
        "                    s = self.scores[i]\n",
        "                    unfinished.append((s, len(self.nextYs) - 1, i)) \n",
        "            unfinished.sort(key=lambda a: -a[0])\n",
        "            self.finished+=unfinished[:self.size-len(self.finished)]\n",
        "        return self.finished[:self.size]\n",
        "\n",
        "    def getHyp(self, beam_res):\n",
        "        \"\"\"\n",
        "        Walk back to construct the full hypothesis.\n",
        "        \"\"\"\n",
        "        hyps=[]\n",
        "        for _,timestep, k in beam_res:\n",
        "            hyp = []\n",
        "            for j in range(len(self.prevKs[:timestep]) - 1, -1, -1):\n",
        "                hyp.append(self.nextYs[j+1][k])\n",
        "                k = self.prevKs[j][k]\n",
        "            hyps.append(hyp[::-1])\n",
        "        return hyps\n",
        "    \n",
        "    def buildTargetTokens(self, preds):\n",
        "        sentence=[]\n",
        "        for pred in preds:\n",
        "            tokens = []\n",
        "            for tok in pred:\n",
        "                if tok==self._eos:\n",
        "                    break\n",
        "                tokens.append(tok)\n",
        "            sentence.append(tokens)\n",
        "        return sentence\n",
        "        \n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrH5YUx6IvWz"
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"\n",
        "Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\n",
        "GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\n",
        "using a masked language modeling (MLM) loss.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "import os\n",
        "import sys\n",
        "import bleu\n",
        "import pickle\n",
        "import torch\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "# import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from io import open\n",
        "from itertools import cycle\n",
        "import torch.nn as nn\n",
        "# from model import Seq2Seq\n",
        "from tqdm import tqdm, trange\n",
        "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
        "                          RobertaConfig, RobertaModel, RobertaTokenizer)\n",
        "MODEL_CLASSES = {'roberta': (RobertaConfig, RobertaModel, RobertaTokenizer)}\n",
        "\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class Example(object):\n",
        "    \"\"\"A single training/test example.\"\"\"\n",
        "    def __init__(self,\n",
        "                 idx,\n",
        "                 source,\n",
        "                 target,\n",
        "                 ):\n",
        "        self.idx = idx\n",
        "        self.source = source\n",
        "        self.target = target\n",
        "\n",
        "def read_examples(filename):\n",
        "    \"\"\"Read examples from filename.\"\"\"\n",
        "    examples=[]\n",
        "\n",
        "    ## open DF, convert to json, iterate one by one\n",
        "    # with open(filename,encoding=\"utf-8\") as f:\n",
        "    #     for idx, line in enumerate(f):\n",
        "    #         line=line.strip()\n",
        "    #         js=json.loads(line)\n",
        "    #         if 'idx' not in js:\n",
        "    #             js['idx']=idx\n",
        "    #         sar=' '.join(js['SAR']).replace('\\n',' ')\n",
        "    #         sar=' '.join(code.strip().split())\n",
        "    #         nl=' '.join(js['NL']).replace('\\n','')\n",
        "    #         nl=' '.join(nl.strip().split())  \n",
        "\n",
        "    df = pd.read_csv(filename)\n",
        "    data_list = list(df.T.to_dict().values())\n",
        "    for idx, data in enumerate(data_list):\n",
        "      nl = data['NL']\n",
        "      sar = data['SAR']\n",
        "      examples.append(\n",
        "          Example(\n",
        "                  idx = idx,\n",
        "                  source=nl,\n",
        "                  target=sar,\n",
        "                  ) \n",
        "      )\n",
        "    return examples\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single training/test features for a example.\"\"\"\n",
        "    def __init__(self,\n",
        "                 example_id,\n",
        "                 source_ids,\n",
        "                 target_ids,\n",
        "                 source_mask,\n",
        "                 target_mask,\n",
        "\n",
        "    ):\n",
        "        self.example_id = example_id\n",
        "        self.source_ids = source_ids\n",
        "        self.target_ids = target_ids\n",
        "        self.source_mask = source_mask\n",
        "        self.target_mask = target_mask       \n",
        "        \n",
        "\n",
        "\n",
        "def convert_examples_to_features(examples, tokenizer, args,stage=None): # MH: make it encoder_tokenizer, decoder_tokenizer\n",
        "    features = []\n",
        "    for example_index, example in enumerate(examples):\n",
        "        #source\n",
        "        source_tokens = tokenizer.tokenize(example.source)[:args.max_source_length-2]\n",
        "        source_tokens =[tokenizer.cls_token]+source_tokens+[tokenizer.sep_token]\n",
        "        source_ids =  tokenizer.convert_tokens_to_ids(source_tokens) \n",
        "        source_mask = [1] * (len(source_tokens))\n",
        "        padding_length = args.max_source_length - len(source_ids)\n",
        "        source_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "        source_mask+=[0]*padding_length\n",
        "\n",
        "        #target\n",
        "        if stage==\"test\":\n",
        "            target_tokens = ['None'] #tokenizer.tokenize(\"None\")\n",
        "        else:\n",
        "            # target_tokens = tokenizer.tokenize(example.target)[:args.max_target_length-2]\n",
        "            target_tokens = example.target.split()[:args.max_target_length-2]\n",
        "        target_tokens = [tokenizer.cls_token]+target_tokens+[tokenizer.sep_token]            \n",
        "        # target_ids = tokenizer.convert_tokens_to_ids(target_tokens) # MH: decoder\n",
        "        target_ids = decoder_tokenizer.convert_string_to_ids(' '.join(target_tokens))\n",
        "        target_mask = [1] *len(target_ids)\n",
        "        padding_length = args.max_target_length - len(target_ids)\n",
        "        target_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "        target_mask+=[0]*padding_length   \n",
        "\n",
        "        if example_index < 5:\n",
        "            if stage=='train':\n",
        "                print(\"*** Example ***\")\n",
        "                print(\"idx: {}\".format(example.idx))\n",
        "\n",
        "                print(\"source_tokens: {}\".format([x.replace('\\u0120','_') for x in source_tokens]))\n",
        "                print(\"source_ids: {}\".format(' '.join(map(str, source_ids))))\n",
        "                print(\"source_mask: {}\".format(' '.join(map(str, source_mask))))\n",
        "                \n",
        "                print(\"target_tokens: {}\".format([x.replace('\\u0120','_') for x in target_tokens]))\n",
        "                print(\"target_ids: {}\".format(' '.join(map(str, target_ids))))\n",
        "                print(\"target_mask: {}\".format(' '.join(map(str, target_mask))))\n",
        "       \n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                 example_index,\n",
        "                 source_ids,\n",
        "                 target_ids,\n",
        "                 source_mask,\n",
        "                 target_mask,\n",
        "            )\n",
        "        )\n",
        "    return features\n",
        "\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYHTONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    \n",
        "\n",
        "def main():\n",
        "\n",
        "    print(args)\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(args.seed)\n",
        "    # make dir if output_dir not exist\n",
        "    if os.path.exists(args.output_dir) is False:\n",
        "        os.makedirs(args.output_dir)\n",
        "        \n",
        "## model was here\n",
        "\n",
        "    if args.do_train:\n",
        "        print(\"Inside TRAIN\")\n",
        "        # Prepare training data loader\n",
        "        train_examples = read_examples(args.train_filename)\n",
        "        train_features = convert_examples_to_features(train_examples, tokenizer,args,stage='train') # MH: 2 tokenizers\n",
        "        all_source_ids = torch.tensor([f.source_ids for f in train_features], dtype=torch.long)\n",
        "        all_source_mask = torch.tensor([f.source_mask for f in train_features], dtype=torch.long)\n",
        "        all_target_ids = torch.tensor([f.target_ids for f in train_features], dtype=torch.long)\n",
        "        all_target_mask = torch.tensor([f.target_mask for f in train_features], dtype=torch.long)    \n",
        "        train_data = TensorDataset(all_source_ids,all_source_mask,all_target_ids,all_target_mask)\n",
        "        \n",
        "        if args.local_rank == -1:\n",
        "            train_sampler = RandomSampler(train_data)\n",
        "        else:\n",
        "            train_sampler = DistributedSampler(train_data)\n",
        "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size//args.gradient_accumulation_steps)\n",
        "\n",
        "        num_train_optimization_steps =  args.train_steps\n",
        "\n",
        "        # Prepare optimizer and schedule (linear warmup and decay)\n",
        "        no_decay = ['bias', 'LayerNorm.weight']\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay': args.weight_decay},\n",
        "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                    num_warmup_steps=int(t_total*0.1),\n",
        "                                                    num_training_steps=t_total)\n",
        "    \n",
        "        #Start training\n",
        "        print(\"***** Running training *****\")\n",
        "        print(\"  Num examples = %d\", len(train_examples))\n",
        "        print(\"  Batch size = %d\", args.train_batch_size)\n",
        "        print(\"  Num epoch = %d\", args.num_train_epochs)\n",
        "        \n",
        "\n",
        "        model.train()\n",
        "        dev_dataset={}\n",
        "        nb_tr_examples, nb_tr_steps,tr_loss,global_step,best_bleu,best_loss = 0, 0,0,0,0,1e6 \n",
        "        for epoch in range(args.num_train_epochs):\n",
        "            bar = tqdm(train_dataloader,total=len(train_dataloader))\n",
        "            for batch in bar:\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                source_ids,source_mask,target_ids,target_mask = batch\n",
        "                loss,_,_ = model(source_ids=source_ids,source_mask=source_mask,target_ids=target_ids,target_mask=target_mask)\n",
        "\n",
        "                if args.n_gpu > 1:\n",
        "                    loss = loss.mean() # mean() to average on multi-gpu.\n",
        "                if args.gradient_accumulation_steps > 1:\n",
        "                    loss = loss / args.gradient_accumulation_steps\n",
        "                tr_loss += loss.item()\n",
        "                train_loss=round(tr_loss*args.gradient_accumulation_steps/(nb_tr_steps+1),4)\n",
        "                bar.set_description(\"epoch {} loss {}\".format(epoch,train_loss))\n",
        "                nb_tr_examples += source_ids.size(0)\n",
        "                nb_tr_steps += 1\n",
        "                loss.backward()\n",
        "\n",
        "                if (nb_tr_steps + 1) % args.gradient_accumulation_steps == 0:\n",
        "                    #Update parameters\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "                    scheduler.step()\n",
        "                    global_step += 1\n",
        "\n",
        "            if args.do_eval:\n",
        "                print(\"Inside EVAL\")\n",
        "                #Eval model with dev dataset\n",
        "                tr_loss = 0\n",
        "                nb_tr_examples, nb_tr_steps = 0, 0                     \n",
        "                eval_flag=False    \n",
        "                if 'dev_loss' in dev_dataset:\n",
        "                    eval_examples,eval_data=dev_dataset['dev_loss']\n",
        "                else:\n",
        "                    eval_examples = read_examples(args.dev_filename)\n",
        "                    eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='dev')\n",
        "                    all_source_ids = torch.tensor([f.source_ids for f in eval_features], dtype=torch.long)\n",
        "                    all_source_mask = torch.tensor([f.source_mask for f in eval_features], dtype=torch.long)\n",
        "                    all_target_ids = torch.tensor([f.target_ids for f in eval_features], dtype=torch.long)\n",
        "                    all_target_mask = torch.tensor([f.target_mask for f in eval_features], dtype=torch.long)      \n",
        "                    eval_data = TensorDataset(all_source_ids,all_source_mask,all_target_ids,all_target_mask)   \n",
        "                    dev_dataset['dev_loss']=eval_examples,eval_data\n",
        "                eval_sampler = SequentialSampler(eval_data)\n",
        "                eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "                print(\"\\n***** Running evaluation *****\")\n",
        "                print(\"  Num examples = %d\", len(eval_examples))\n",
        "                print(\"  Batch size = %d\", args.eval_batch_size)\n",
        "\n",
        "                #Start Evaling model\n",
        "                model.eval()\n",
        "                eval_loss,tokens_num = 0,0\n",
        "                for batch in eval_dataloader:\n",
        "                    batch = tuple(t.to(device) for t in batch)\n",
        "                    source_ids,source_mask,target_ids,target_mask = batch                  \n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        _,loss,num = model(source_ids=source_ids,source_mask=source_mask,\n",
        "                                           target_ids=target_ids,target_mask=target_mask)     \n",
        "                    eval_loss += loss.sum().item()\n",
        "                    tokens_num += num.sum().item()\n",
        "                #Pring loss of dev dataset    \n",
        "                model.train()\n",
        "                eval_loss = eval_loss / tokens_num\n",
        "                result = {'eval_ppl': round(np.exp(eval_loss),5),\n",
        "                          'global_step': global_step+1,\n",
        "                          'train_loss': round(train_loss,5)}\n",
        "                for key in sorted(result.keys()):\n",
        "                    print(\"  %s = %s\", key, str(result[key]))\n",
        "                print(\"  \"+\"*\"*20)   \n",
        "\n",
        "                #save last checkpoint\n",
        "                last_output_dir = os.path.join(args.output_dir, 'checkpoint-last')\n",
        "                if not os.path.exists(last_output_dir):\n",
        "                    os.makedirs(last_output_dir)\n",
        "                model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "                output_model_file = os.path.join(last_output_dir, \"pytorch_model.bin\")\n",
        "                torch.save(model_to_save.state_dict(), output_model_file)                    \n",
        "                if eval_loss<best_loss:\n",
        "                    print(\"  Best ppl:%s\",round(np.exp(eval_loss),5))\n",
        "                    print(\"  \"+\"*\"*20)\n",
        "                    best_loss=eval_loss\n",
        "                    # Save best checkpoint for best ppl\n",
        "                    output_dir = os.path.join(args.output_dir, 'checkpoint-best-ppl')\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "                    output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
        "                    torch.save(model_to_save.state_dict(), output_model_file)  \n",
        "\n",
        "\n",
        "                #Calculate bleu  \n",
        "                print(\"Calculating BLEU\")\n",
        "                if 'dev_bleu' in dev_dataset:\n",
        "                    eval_examples,eval_data=dev_dataset['dev_bleu']\n",
        "                else:\n",
        "                    eval_examples = read_examples(args.dev_filename)\n",
        "                    eval_examples = random.sample(eval_examples,min(1000,len(eval_examples)))\n",
        "                    eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='test')\n",
        "                    all_source_ids = torch.tensor([f.source_ids for f in eval_features], dtype=torch.long)\n",
        "                    all_source_mask = torch.tensor([f.source_mask for f in eval_features], dtype=torch.long)    \n",
        "                    eval_data = TensorDataset(all_source_ids,all_source_mask)   \n",
        "                    dev_dataset['dev_bleu']=eval_examples,eval_data\n",
        "\n",
        "\n",
        "\n",
        "                eval_sampler = SequentialSampler(eval_data)\n",
        "                eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "                model.eval() \n",
        "                p=[]\n",
        "                for batch in eval_dataloader:\n",
        "                    batch = tuple(t.to(device) for t in batch)\n",
        "                    source_ids,source_mask= batch                  \n",
        "                    with torch.no_grad():\n",
        "                        preds = model(source_ids=source_ids,source_mask=source_mask)  \n",
        "                        for pred in preds:\n",
        "                            t=pred[0].cpu().numpy()\n",
        "                            t=list(t)\n",
        "                            if 0 in t:\n",
        "                                t=t[:t.index(0)]\n",
        "                            # text = tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
        "                            text = decoder_tokenizer.decode(t) # MH: decoder\n",
        "                            p.append(text)\n",
        "                model.train()\n",
        "                predictions=[]\n",
        "                with open(os.path.join(args.output_dir,\"dev.output\"),'w') as f, open(os.path.join(args.output_dir,\"dev.gold\"),'w') as f1:\n",
        "                    for ref,gold in zip(p,eval_examples):\n",
        "                        predictions.append(str(gold.idx)+'\\t'+ref)\n",
        "                        f.write(str(gold.idx)+'\\t'+ref+'\\n')\n",
        "                        f1.write(str(gold.idx)+'\\t'+gold.target+'\\n')     \n",
        "\n",
        "                (goldMap, predictionMap) = bleu.computeMaps(predictions, os.path.join(args.output_dir, \"dev.gold\")) \n",
        "                dev_bleu=round(bleu.bleuFromMaps(goldMap, predictionMap)[0],2)\n",
        "                print(\"  %s = %s \"%(\"bleu-4\",str(dev_bleu)))\n",
        "                print(\"  \"+\"*\"*20)    \n",
        "                if dev_bleu>best_bleu:\n",
        "                    print(\"  Best bleu:%s\",dev_bleu)\n",
        "                    print(\"  \"+\"*\"*20)\n",
        "                    best_bleu=dev_bleu\n",
        "                    # Save best checkpoint for best bleu\n",
        "                    output_dir = os.path.join(args.output_dir, 'checkpoint-best-bleu')\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "                    output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
        "                    torch.save(model_to_save.state_dict(), output_model_file)\n",
        "               \n",
        "    if args.do_test:\n",
        "        # print(\"Inside TEST\")\n",
        "        # files=[]\n",
        "        # if args.dev_filename is not None:\n",
        "        #     files.append(args.dev_filename)\n",
        "        # if args.test_filename is not None:\n",
        "        #     files.append(args.test_filename)\n",
        "        # for idx,file in enumerate(files):   \n",
        "        idx = 0\n",
        "        file = args.test_filename # Change it to test file later\n",
        "        print(\"Test file: {}\".format(file))\n",
        "        eval_examples = read_examples(file)\n",
        "        eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='test')\n",
        "        all_source_ids = torch.tensor([f.source_ids for f in eval_features], dtype=torch.long)\n",
        "        all_source_mask = torch.tensor([f.source_mask for f in eval_features], dtype=torch.long)    \n",
        "        eval_data = TensorDataset(all_source_ids,all_source_mask)   \n",
        "\n",
        "        # Calculate bleu\n",
        "        eval_sampler = SequentialSampler(eval_data)\n",
        "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "        model.eval() \n",
        "        p=[]\n",
        "        for batch in tqdm(eval_dataloader,total=len(eval_dataloader)):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            source_ids,source_mask= batch                  \n",
        "            with torch.no_grad():\n",
        "                preds = model(source_ids=source_ids,source_mask=source_mask) \n",
        "                for pred in preds:\n",
        "                    t=pred[0].cpu().numpy()\n",
        "                    t=list(t)\n",
        "                    if 0 in t:\n",
        "                        t=t[:t.index(0)]\n",
        "                    # text = tokenizer.decode(t,clean_up_tokenization_spaces=False) # MH: decoder\n",
        "                    text = decoder_tokenizer.decode(t)\n",
        "                    p.append(text)\n",
        "        model.train()\n",
        "        predictions=[]\n",
        "        with open(os.path.join(args.output_dir,\"test_{}.output\".format(str(idx))),'w') as f, open(os.path.join(args.output_dir,\"test_{}.gold\".format(str(idx))),'w') as f1:\n",
        "            for ref,gold in zip(p,eval_examples):\n",
        "                predictions.append(str(gold.idx)+'\\t'+ref)\n",
        "                f.write(str(gold.idx)+'\\t'+ref+'\\n')\n",
        "                f1.write(str(gold.idx)+'\\t'+gold.target+'\\n')     \n",
        "\n",
        "        (goldMap, predictionMap) = bleu.computeMaps(predictions, os.path.join(args.output_dir, args.output_name+\"test_{}.gold\".format(idx))) \n",
        "        dev_bleu=round(bleu.bleuFromMaps(goldMap, predictionMap)[0],2)\n",
        "        print(\"  %s = %s \"%(\"bleu-4\",str(dev_bleu)))\n",
        "        print(\"  \"+\"*\"*20)\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffH7YvV6Oaxz"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34alizypPOH9"
      },
      "source": [
        "class Arguments:\n",
        "    pass"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAaxEIuUM2uG"
      },
      "source": [
        "output_dir=\"/gdrive/My Drive/text2app_models/RoBERTa/\" # Colab + Drive\n",
        "# output_dir=\"model/\" # Local\n",
        "data_dir = '../synthesized_data/'\n",
        "train_file=data_dir+'nl_sar_train.csv'\n",
        "dev_file=data_dir+'nl_sar_valid.csv'\n",
        "test_file=data_dir+'nl_sar_test.csv'\n",
        "pretrained_model= 'roberta-base' #'microsoft/codebert-base' #'roberta-base'"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpOUBhufLt-w"
      },
      "source": [
        "args = Arguments()\n",
        "\n",
        "## Required parameters\n",
        "args.model_type='roberta'\n",
        "args.model_name_or_path=pretrained_model\n",
        "args.output_dir=output_dir\n",
        "args.load_model_path=None #output_dir+\"/checkpoint-best-bleu/pytorch_model.bin\"\n",
        "## Other parameters\n",
        "args.train_filename=train_file\n",
        "args.dev_filename=dev_file\n",
        "args.test_filename=test_file\n",
        "args.output_name = \"\"\n",
        "args.config_name=\"\"\n",
        "args.tokenizer_name=\"\"\n",
        "args.gradient_accumulation_steps=1\n",
        "args.weight_decay=0.0\n",
        "args.adam_epsilon=1e-8\n",
        "args.max_grad_norm=1.0\n",
        "args.max_steps=-1\n",
        "args.eval_steps=-1\n",
        "args.train_steps=-1\n",
        "args.warmup_steps=0\n",
        "args.local_rank=-1\n",
        "args.seed=42\n",
        "\n",
        "args.no_cuda=False\n",
        "args.do_lower_case=True\n",
        "args.do_train=True\n",
        "args.do_eval=True\n",
        "args.do_test=False\n",
        "\n",
        "args.num_train_epochs=6\n",
        "args.train_batch_size=100\n",
        "args.eval_batch_size=100\n",
        "args.learning_rate=5e-5\n",
        "args.max_source_length=80\n",
        "args.max_target_length=50\n",
        "args.beam_size=5"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSceeljfzY0I",
        "outputId": "8728f966-1068-441a-c737-ffb688923411"
      },
      "source": [
        "# Setup CUDA, GPU & distributed training\n",
        "if args.local_rank == -1 or args.no_cuda:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "    args.n_gpu = torch.cuda.device_count()\n",
        "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "    torch.cuda.set_device(args.local_rank)\n",
        "    device = torch.device(\"cuda\", args.local_rank)\n",
        "    torch.distributed.init_process_group(backend='nccl')\n",
        "    args.n_gpu = 1\n",
        "logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s\",\n",
        "                args.local_rank, device, args.n_gpu, bool(args.local_rank != -1))\n",
        "args.device = device"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "07/01/2021 17:16:13 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YVnN-E_zEcj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461,
          "referenced_widgets": [
            "292024ba0d0945d692ff6b0cb0f83df4",
            "a3eda0b179ba421ca951dde0d74e07a4",
            "c9809e49e903489b8f89cd5706ccd9f4",
            "62153797884e4062b66b0ce321a838d3",
            "55cb5c86b1d24ed789001115f6d38b67",
            "78d92543fe054c11a364371b768ec31b",
            "13cba081479c435d9a84d2d22171fd5e",
            "cd9e7c6805534330b7d6bea55428a0c9",
            "93d52f92c87442a5a1c9e31a2479a5c9",
            "b22a9ab3e86546b1a1c543563e2a9257",
            "91811922ac8b4002b415c823d4760ed9",
            "433059e4d86e4475ad1727a97528eef2",
            "7bc22a9f81944aebb211cb35df287248",
            "4f79e77351d64b9491336234c50b5ef3",
            "bdf166241e81437497e683493650c8c8",
            "bc1b819316764aa59f3ebe14cfadd716",
            "344264c011a346e8965e87d5d902e444",
            "2a51d81c2bee437b838f78337d0cb8d7",
            "eb911ccbe057437f9181d9422b0ce02e",
            "27e52256b8a943e590f24e34497c2a12",
            "063851eec58a47e0bacdfab19af71dc0",
            "62f669d89b0646e094110ce4fcc3bc21",
            "b99e680b5376497e905104a9f4176ffb",
            "dcde829fe15f48dbb71855ef1106e8f9",
            "dadca3763a1e456daea6f92aa4cff30b",
            "54d687ad53324815a870235b5e339602",
            "cf4193b56a8042fa8ba276e8b3eb9b0f",
            "9d79a27651a44658a98435690b8d4c45",
            "3f7b4f0877ad4f50aedfbc9f0ee44088",
            "33cd2e5f093945d58864252556fb3ff7",
            "9c2858131bf649b08346347d23fb2756",
            "d8c433dbce624f6fa74b35eabb187f2f",
            "182d806726524994a06ee960b78a790b",
            "7a2eeb308d234ca0bf0c0f55f5a3c82b",
            "155c7edb8bbf43649f58de02dfbe1ce3",
            "b644a64c67c446409ea33015754a0d4e",
            "1dcccd50354b40e48571ad0b491681a2",
            "c5630e809a2148c58af29d3834dd63fb",
            "db49c8b057a44332837b3a03b7ddabe8",
            "d6d9118742fe4b84a7646d7450e713ae"
          ]
        },
        "outputId": "71340459-8cb0-4b06-845c-adbaf1945275"
      },
      "source": [
        "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
        "config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path)\n",
        "tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,do_lower_case=args.do_lower_case)\n",
        "\n",
        "decoder_tokenizer = MyTokenizer()\n",
        "\n",
        "#budild model\n",
        "encoder = model_class.from_pretrained(args.model_name_or_path,config=config)    \n",
        "decoder_layer = nn.TransformerDecoderLayer(d_model=config.hidden_size, nhead=config.num_attention_heads)\n",
        "decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "07/01/2021 17:16:39 - INFO - filelock -   Lock 140202383649168 acquired on /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "292024ba0d0945d692ff6b0cb0f83df4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=481.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "07/01/2021 17:16:39 - INFO - filelock -   Lock 140202383649168 released on /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "07/01/2021 17:16:39 - INFO - filelock -   Lock 140201252258384 acquired on /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93d52f92c87442a5a1c9e31a2479a5c9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "07/01/2021 17:16:40 - INFO - filelock -   Lock 140201252258384 released on /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "07/01/2021 17:16:40 - INFO - filelock -   Lock 140201248528272 acquired on /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "344264c011a346e8965e87d5d902e444",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "07/01/2021 17:16:41 - INFO - filelock -   Lock 140201248528272 released on /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "07/01/2021 17:16:42 - INFO - filelock -   Lock 140201248528272 acquired on /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dadca3763a1e456daea6f92aa4cff30b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355863.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "07/01/2021 17:16:43 - INFO - filelock -   Lock 140201248528272 released on /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "07/01/2021 17:16:43 - INFO - filelock -   Lock 140201194338064 acquired on /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "182d806726524994a06ee960b78a790b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=501200538.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "07/01/2021 17:16:54 - INFO - filelock -   Lock 140201194338064 released on /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_SNN0D9EB8p"
      },
      "source": [
        "model=Seq2Seq(encoder=encoder,decoder=decoder,config=config,\n",
        "              beam_size=args.beam_size,max_length=args.max_target_length,\n",
        "              sos_id=tokenizer.cls_token_id,eos_id=tokenizer.sep_token_id)\n",
        "if args.load_model_path is not None:\n",
        "    print(\"reload model from {}\".format(args.load_model_path))\n",
        "    model.load_state_dict(torch.load(args.load_model_path))\n",
        "    \n",
        "model.to(device)\n",
        "if args.local_rank != -1:\n",
        "    # Distributed training\n",
        "    try:\n",
        "        from apex.parallel import DistributedDataParallel as DDP\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
        "    model = DDP(model)\n",
        "elif args.n_gpu > 1:\n",
        "    # multi-gpu training\n",
        "    model = torch.nn.DataParallel(model)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8fS_vh0I0P5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c9b0a7a-8606-4f0f-9afe-78a91b62d0e9"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.Arguments object at 0x7fd98d781fd0>\n",
            "Inside TRAIN\n",
            "*** Example ***\n",
            "idx: 0\n",
            "source_tokens: ['<s>', 'create', '_a', '_mobile', '_app', '_with', '_a', '_text', '2', 'speech', '_,', '_a', '_button', '_for', '_a', '_label', '_,', '_a', '_text', 'box', '_,', '_a', '_switch', '_,', '_optional', '_password', 'text', 'box', '_,', '_an', '_audio', '_with', '_source', '_string', '0', '_,', '_and', '_a', '_time', '_pick', 'er', '_.', '_if', '_the', '_button', '_is', '_touched', ',', '_set', '_the', '_label', '_adjacent', '_to', '_the', '_text', '_in', '_the', '_box', '_.', '_if', '_the', '_switch', '_is', '_pressed', ',', '_restart', '_the', '_player', '_.', '</s>']\n",
            "source_ids: 0 32845 10 1830 1553 19 10 2788 176 40511 2156 10 6148 13 10 6929 2156 10 2788 8304 2156 10 5405 2156 17679 14844 29015 8304 2156 41 6086 19 1300 6755 288 2156 8 10 86 1339 254 479 114 5 6148 16 6699 6 278 5 6929 12142 7 5 2788 11 5 2233 479 114 5 5405 16 11224 6 12721 5 869 479 2 1 1 1 1 1 1 1 1 1 1\n",
            "source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
            "target_tokens: ['<s>', '<complist>', '<text2speech>', '<button>', 'button1', '</button>', '<label>', 'label1', '</label>', '<textbox>', '<switch>', 'switch1', '</switch>', '<passwordtextbox>', '<player>', 'string0', '</player>', '<timepicker>', '</complist>', '<code>', '<button1clicked>', '<label1>', '<textboxtext1>', '</label>', '</button1clicked>', '<switch1flipped>', '<player1>', '<stop>', '</player>', '</switch1flipped>', '</code>', '</s>']\n",
            "target_ids: 0 50 82 44 100 13 59 105 19 83 80 123 27 64 70 113 20 93 17 48 39 56 84 19 8 76 66 75 20 23 15 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "*** Example ***\n",
            "idx: 1\n",
            "source_tokens: ['<s>', 'save', '_me', '_!', '_mobile', '_app', '_includes', '_a', '_text', 'box', '_,', '_a', '_video', '_with', '_source', '_string', '0', '_,', '_and', '_a', '_circle', '_.', '_if', '_the', '_circle', '_is', '_touches', '_edge', ',', '_stop', '_video', '_.', '</s>']\n",
            "source_ids: 0 31575 162 27785 1830 1553 1171 10 2788 8304 2156 10 569 19 1300 6755 288 2156 8 10 7922 479 114 5 7922 16 12325 3543 6 912 569 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "target_tokens: ['<s>', '<complist>', '<textbox>', '<video_player>', 'string0', '</video_player>', '<ball>', '</complist>', '<code>', '<ball1reach_edge>', '<video_player1>', '<stop>', '</video_player>', '</ball1reach_edge>', '</code>', '</s>']\n",
            "target_ids: 0 50 83 98 113 29 35 17 48 34 94 75 29 7 15 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "*** Example ***\n",
            "idx: 2\n",
            "source_tokens: ['<s>', 'make', '_mobile', '_application', '_that', '_requires', '_a', '_camera', '_,', '_a', '_text', '_box', '_,', '_another', '_text', '2', 'speech', '_,', '_a', '_video', '_box', '_with', '_source', '_string', '0', '_,', '_a', '_circle', '_,', '_a', '_motion', '_sensor', '_,', '_a', '_pas', 'word', '_text', '_box', '_,', '_a', '_label', '_,', '_a', '_video', '_,', '_a', '_random', '_clip', '_,', '_a', '_switch', '_,', '_and', '_a', '_video', '_with', '_source', '_string', '1', '_.', '_when', '_the', '_circle', '_is', '_touches', '_please', '_tell', '_the', '_text', '_in', '_the', '_text', 'box', '_.', '_if', '_the', '_acceler', 'ometer', '_is', '</s>']\n",
            "source_ids: 0 19746 1830 2502 14 3441 10 2280 2156 10 2788 2233 2156 277 2788 176 40511 2156 10 569 2233 19 1300 6755 288 2156 10 7922 2156 10 4298 9626 2156 10 6977 14742 2788 2233 2156 10 6929 2156 10 569 2156 10 9624 7200 2156 10 5405 2156 8 10 569 19 1300 6755 134 479 77 5 7922 16 12325 2540 1137 5 2788 11 5 2788 8304 479 114 5 27416 12687 16 2\n",
            "source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "target_tokens: ['<s>', '<complist>', '<camera>', '<textbox>', '<text2speech>', '<video_player>', 'string0', '</video_player>', '<ball>', '<accelerometer>', '<passwordtextbox>', '<label>', 'label1', '</label>', '<video_player>', 'random_video_player_source', '</video_player>', '<switch>', 'switch1', '</switch>', '<video_player>', 'string1', '</video_player>', '</complist>', '<code>', '<ball1flung>', '<text2speech1>', '<textboxtext1>', '</text2speech1>', '</ball1flung>', '<accelerometer1shaken>', '<video_player1>', '<start>', '</video_player>', '</accelerometer1shaken>', '<switch1flipped>', '<video_player2>', '<start>', '</video_player>', '</switch1flipped>', '</code>', '</s>']\n",
            "target_ids: 0 50 46 83 82 98 113 29 35 31 64 59 105 19 98 112 29 80 123 27 98 114 29 17 48 33 81 84 28 6 30 94 74 29 4 76 95 74 29 23 15 2 1 1 1 1 1 1 1 1\n",
            "target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
            "*** Example ***\n",
            "idx: 3\n",
            "source_tokens: ['<s>', 'mobile', '_app', '_containing', '_a', '_movement', '_sensor', '_,', '_a', '_microphone', '_,', '_and', '_a', '_speech', '_.', '_when', '_the', '_shake', '_knob', '_is', '_shaken', ',', '_speak', '_the', '_text', 'box', '_content', '_.', '</s>']\n",
            "source_ids: 0 25254 1553 8200 10 2079 9626 2156 10 18896 2156 8 10 1901 479 77 5 8559 42256 16 17548 6 1994 5 2788 8304 1383 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "target_tokens: ['<s>', '<complist>', '<accelerometer>', '<textbox>', '<text2speech>', '</complist>', '<code>', '<accelerometer1shaken>', '<text2speech1>', '<textboxtext1>', '</text2speech1>', '</accelerometer1shaken>', '</code>', '</s>']\n",
            "target_ids: 0 50 31 83 82 17 48 30 81 84 28 4 15 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "*** Example ***\n",
            "idx: 4\n",
            "source_tokens: ['<s>', 'create', '_a', '_mobile', '_app', '_containing', '_a', '_pas', 'word', '_text', '_box', '_;', '_a', '_string', '_,', '_a', '_label', '_,', '_a', '_button', '_called', '_string', '0', '_,', '_a', '_label', '_named', '_string', '1', '_,', '_a', '_button', '_,', '_a', '_ball', '_,', '_and', '_a', '_time', '_pick', 'er', '_.', '_whenever', '_the', '_switch', '_is', '_clicked', ',', '_set', '_label', '_text', '_to', '_the', '_time', '_.', '_if', '_the', '_initial', '_button', '_is', '_clicked', ',', '_set', '_label', '_string', '2', '_.', '_when', '_clicked', '_the', '_later', '_button', '_is', '_flipped', '_set', '_the', '_speed', '_to', '_number', '</s>']\n",
            "source_ids: 0 32845 10 1830 1553 8200 10 6977 14742 2788 2233 25606 10 6755 2156 10 6929 2156 10 6148 373 6755 288 2156 10 6929 1440 6755 134 2156 10 6148 2156 10 1011 2156 8 10 86 1339 254 479 8378 5 5405 16 28551 6 278 6929 2788 7 5 86 479 114 5 2557 6148 16 28551 6 278 6929 6755 176 479 77 28551 5 423 6148 16 18626 278 5 2078 7 346 2\n",
            "source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "target_tokens: ['<s>', '<complist>', '<passwordtextbox>', '<switch>', 'switch1', '</switch>', '<label>', 'label1', '</label>', '<button>', 'string0', '</button>', '<label>', 'string1', '</label>', '<button>', 'button2', '</button>', '<ball>', '<timepicker>', '</complist>', '<code>', '<switch1flipped>', '<label1>', '<time1>', '</label>', '</switch1flipped>', '<button1clicked>', '<label2>', 'string2', '</label>', '</button1clicked>', '<button2clicked>', '<ball1>', '<speed>', 'number0', '</speed>', '</ball1>', '</button2clicked>', '</code>', '</s>']\n",
            "target_ids: 0 50 64 80 123 27 59 105 19 44 113 13 59 114 19 44 101 13 35 93 17 48 76 56 88 19 23 39 57 115 19 8 40 32 73 110 22 5 9 15 2 1 1 1 1 1 1 1 1 1\n",
            "target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/755 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = %d 75449\n",
            "  Batch size = %d 100\n",
            "  Num epoch = %d 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0 loss 1.3988: 100%|██████████| 755/755 [25:26<00:00,  2.02s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inside EVAL\n",
            "\n",
            "***** Running evaluation *****\n",
            "  Num examples = %d 9431\n",
            "  Batch size = %d 100\n",
            "  %s = %s eval_ppl 1.20398\n",
            "  %s = %s global_step 756\n",
            "  %s = %s train_loss 1.3988\n",
            "  ********************\n",
            "  Best ppl:%s 1.20398\n",
            "  ********************\n",
            "Calculating BLEU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            "Total: 1000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  bleu-4 = 92.68 \n",
            "  ********************\n",
            "  Best bleu:%s 92.68\n",
            "  ********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1 loss 0.1446: 100%|██████████| 755/755 [25:22<00:00,  2.02s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inside EVAL\n",
            "\n",
            "***** Running evaluation *****\n",
            "  Num examples = %d 9431\n",
            "  Batch size = %d 100\n",
            "  %s = %s eval_ppl 1.09597\n",
            "  %s = %s global_step 1511\n",
            "  %s = %s train_loss 0.1446\n",
            "  ********************\n",
            "  Best ppl:%s 1.09597\n",
            "  ********************\n",
            "Calculating BLEU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Total: 1000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  bleu-4 = 94.67 \n",
            "  ********************\n",
            "  Best bleu:%s 94.67\n",
            "  ********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 2 loss 0.0914: 100%|██████████| 755/755 [25:23<00:00,  2.02s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inside EVAL\n",
            "\n",
            "***** Running evaluation *****\n",
            "  Num examples = %d 9431\n",
            "  Batch size = %d 100\n",
            "  %s = %s eval_ppl 1.08296\n",
            "  %s = %s global_step 2266\n",
            "  %s = %s train_loss 0.0914\n",
            "  ********************\n",
            "  Best ppl:%s 1.08296\n",
            "  ********************\n",
            "Calculating BLEU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Total: 1000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  bleu-4 = 95.06 \n",
            "  ********************\n",
            "  Best bleu:%s 95.06\n",
            "  ********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 3 loss 0.0794: 100%|██████████| 755/755 [25:26<00:00,  2.02s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inside EVAL\n",
            "\n",
            "***** Running evaluation *****\n",
            "  Num examples = %d 9431\n",
            "  Batch size = %d 100\n",
            "  %s = %s eval_ppl 1.07785\n",
            "  %s = %s global_step 3021\n",
            "  %s = %s train_loss 0.0794\n",
            "  ********************\n",
            "  Best ppl:%s 1.07785\n",
            "  ********************\n",
            "Calculating BLEU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Total: 1000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  bleu-4 = 95.21 \n",
            "  ********************\n",
            "  Best bleu:%s 95.21\n",
            "  ********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 4 loss 0.0722: 100%|██████████| 755/755 [25:25<00:00,  2.02s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inside EVAL\n",
            "\n",
            "***** Running evaluation *****\n",
            "  Num examples = %d 9431\n",
            "  Batch size = %d 100\n",
            "  %s = %s eval_ppl 1.0742\n",
            "  %s = %s global_step 3776\n",
            "  %s = %s train_loss 0.0722\n",
            "  ********************\n",
            "  Best ppl:%s 1.0742\n",
            "  ********************\n",
            "Calculating BLEU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Total: 1000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  bleu-4 = 95.64 \n",
            "  ********************\n",
            "  Best bleu:%s 95.64\n",
            "  ********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 5 loss 0.067: 100%|██████████| 755/755 [25:25<00:00,  2.02s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inside EVAL\n",
            "\n",
            "***** Running evaluation *****\n",
            "  Num examples = %d 9431\n",
            "  Batch size = %d 100\n",
            "  %s = %s eval_ppl 1.07156\n",
            "  %s = %s global_step 4531\n",
            "  %s = %s train_loss 0.067\n",
            "  ********************\n",
            "  Best ppl:%s 1.07156\n",
            "  ********************\n",
            "Calculating BLEU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Total: 1000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  bleu-4 = 95.55 \n",
            "  ********************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TptSjxB5Ye96"
      },
      "source": [
        "# If working on colab, save checkpoint to Google Drive\n",
        "# !cp model/checkpoint-best-bleu/pytorch_model.bin '/gdrive/My Drive/text2app_models/RoBERTa/' # CodeBERT RoBERTa PointerNet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIV6rQ0oOUPd"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-V6r9YtBOPWG"
      },
      "source": [
        "args.do_test=True\n",
        "args.do_train=False\n",
        "args.beam_size=1\n",
        "# args.test_filename=data_dir+'unseen_test.csv'\n",
        "# args.output_name = \"unseen_pair_\""
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LDVxq3eJ4Xf"
      },
      "source": [
        "**Test Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5Do7psBVMLW"
      },
      "source": [
        "model=Seq2Seq(encoder=encoder,decoder=decoder,config=config,\n",
        "              beam_size=args.beam_size,max_length=args.max_target_length,\n",
        "              sos_id=tokenizer.cls_token_id,eos_id=tokenizer.sep_token_id)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUa9ag27Dyms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af924bb7-e0e2-473d-80bf-db8738868e0b"
      },
      "source": [
        "# args.load_model_path=output_dir+\"/checkpoint-best-bleu/pytorch_model.bin\"\n",
        "args.load_model_path='/gdrive/My Drive/text2app_models/RoBERTa/checkpoint-best-bleu/roberta.bin'\n",
        "\n",
        "model.load_state_dict(torch.load(args.load_model_path))\n",
        "model.to(device)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (decoder): TransformerDecoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (3): TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (4): TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (5): TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (lm_head): Linear(in_features=768, out_features=129, bias=False)\n",
              "  (lsm): LogSoftmax(dim=-1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viAeasFHE5dG"
      },
      "source": [
        "test[:5].to_csv('../synthesized_data/tiny_test.csv')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95HrLOBKFQaD"
      },
      "source": [
        "!mkdir '/gdrive/My Drive/text2app_models/RoBERTa/tiny_test/'"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuOgSNYxHSVK",
        "outputId": "91b72052-e7ee-49c0-e9ea-6ed1b59b4e9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "args.output_dir"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/gdrive/My Drive/text2app_models/RoBERTa/'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x15hh5QgBweO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18d93a28-972b-4665-c91b-f66fd9341fce"
      },
      "source": [
        "test_files = ['tiny_test.csv'] #'unseen_test.csv']\n",
        "              # , 'nl_sar_test_2%_mutation.csv', 'nl_sar_test_5%_mutation.csv', \n",
        "              #     'nl_sar_test_10%_mutation.csv', 'nl_sar_test_unseen_pair.csv']\n",
        "\n",
        "for test_file in test_files:\n",
        "  args.test_filename=data_dir+test_file\n",
        "  args.output_name=test_file[:-4]+\"/\"\n",
        "  # args.output_dir += test_file[:-4]+\"/\"\n",
        "  print(\"#### testing\", test_file)\n",
        "  path = args.output_dir+args.output_name\n",
        "  if not os.path.exists(path):\n",
        "    !mkdir $path\n",
        "\n",
        "  if __name__ == \"__main__\":\n",
        "      main()\n",
        "\n",
        "  ref = open(args.output_dir+\"test_0.gold\")\n",
        "  ref_sar = ref.readlines()\n",
        "  ref.close()\n",
        "  pred = open(args.output_dir+\"test_0.output\")\n",
        "  pred_sar = pred.readlines()\n",
        "  pred.close()\n",
        "  correct = 0\n",
        "  for i in range(len(ref_sar)):\n",
        "    if ref_sar[i]==pred_sar[i]:\n",
        "      correct+=1\n",
        "  print(\"Exact Match: \", 100*correct/len(ref_sar))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "#### testing tiny_test.csv\n",
            "<__main__.Arguments object at 0x7f8322d20590>\n",
            "Test file: ../synthesized_data/tiny_test.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.02s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  bleu-4 = 96.0 \n",
            "  ********************\n",
            "Exact Match:  40.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Total: 5\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rHSgVpz0Eja"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVNsWW7HQNvs"
      },
      "source": [
        "## Calculate BLEU-4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNOFrVjDQQbr",
        "outputId": "3579d1bc-2320-4787-fad0-3c1b32eab580"
      },
      "source": [
        "!python evaluator.py '/gdrive/My Drive/text2app_models/RoBERTa/nl_sar_test/test_0.gold' < '/gdrive/My Drive/text2app_models/RoBERTa/nl_sar_test/test_0.output'"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total: 9432\n",
            "95.10608029247508\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNhQAwBAtYpo"
      },
      "source": [
        "Test: BLEU: 95.10608029247508 EM: 56.99745547073791\n",
        "\n",
        "Unseen Test: BLEU: 94.89926156734143, EM: 48.497067448680355"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFdkTBf7zHHi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfVbwSg4bnpx"
      },
      "source": [
        "## Exact Match"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paPK1mbrRtmF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2678aca8-8ec5-4d29-834f-cec7e3cafd8b"
      },
      "source": [
        "ref = open(\"model/test_0.gold\")\n",
        "ref_sar = ref.readlines()\n",
        "ref.close()\n",
        "\n",
        "pred = open(\"model/test_0.output\")\n",
        "pred_sar = pred.readlines()\n",
        "pred.close()\n",
        "\n",
        "correct = 0\n",
        "for i in range(len(ref_sar)):\n",
        "  if ref_sar[i]==pred_sar[i]:\n",
        "    correct+=1\n",
        "\n",
        "100*correct/len(ref_sar)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "77.8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l_2F79ey3se"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeHTIMPkQ3Y4"
      },
      "source": [
        "## Single NL prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfpi_W0EdNvZ"
      },
      "source": [
        "def single_example_to_feature(example, tokenizer): # MH: make it encoder_tokenizer, decoder_tokenizer\n",
        "    features = []\n",
        "    source_tokens = tokenizer.tokenize(example)[:args.max_source_length-2]\n",
        "    source_tokens =[tokenizer.cls_token]+source_tokens+[tokenizer.sep_token]\n",
        "    source_ids =  tokenizer.convert_tokens_to_ids(source_tokens) \n",
        "    source_mask = [1] * (len(source_tokens))\n",
        "    padding_length = args.max_source_length - len(source_ids)\n",
        "    source_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "    source_mask+=[0]*padding_length\n",
        " \n",
        "    target_tokens = ['None']\n",
        "    target_tokens = [tokenizer.cls_token]+target_tokens+[tokenizer.sep_token]            \n",
        "    # target_ids = tokenizer.convert_tokens_to_ids(target_tokens) # MH: decoder\n",
        "    target_ids = decoder_tokenizer.convert_string_to_ids(' '.join(target_tokens))\n",
        "    target_mask = [1] *len(target_ids)\n",
        "    padding_length = args.max_target_length - len(target_ids)\n",
        "    target_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "    target_mask+=[0]*padding_length   \n",
        "\n",
        "    features.append(\n",
        "        InputFeatures(\n",
        "              0,\n",
        "              source_ids,\n",
        "              target_ids,\n",
        "              source_mask,\n",
        "              target_mask,\n",
        "        )\n",
        "    )\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJPiYhLiIWgK"
      },
      "source": [
        "args.eval_batch_size = 1\n",
        "args.do_test=True\n",
        "args.do_train=False\n",
        "args.beam_size=1\n",
        "args.load_model_path=output_dir+\"/checkpoint-best-bleu/pytorch_model.bin\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogRkbfcwfDph"
      },
      "source": [
        "model.eval() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldJhnZ92w3aP"
      },
      "source": [
        "def get_sar(eval_example):\n",
        "  eval_features = single_example_to_feature(eval_example, tokenizer)\n",
        "  all_source_ids = torch.tensor([f.source_ids for f in eval_features], dtype=torch.long).to(device)\n",
        "  all_source_mask = torch.tensor([f.source_mask for f in eval_features], dtype=torch.long).to(device)\n",
        "  preds = model(source_ids=all_source_ids, source_mask=all_source_mask)  \n",
        "  pred_list = list(preds[0][0].cpu().numpy())\n",
        "  predicted_text = decoder_tokenizer.decode(pred_list[:pred_list.index(0)])\n",
        "  return predicted_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l40SKW1237Wr"
      },
      "source": [
        "eval_example = 'make an app with a textbox, a button named \"tweet\", and a label. When the button is pressed, set the label to textbox text..'\n",
        "\n",
        "get_sar(eval_example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6iq_DHvHBqJ"
      },
      "source": [
        "\"string0\": go\n",
        "\"string1\": back\n",
        "<complist> <textbox> <button> go </button> <button> back </button> </complist> <code> <button1clicked> <label1> <textboxtext1> </label1> </button1clicked> <button2clicked> <ball1> <color> <gray> </color> </ball1> </button2clicked> </code>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3EpBuayf0dp"
      },
      "source": [
        "get_sar(\"make an app with an accelerometer and a music player . when accelerometer is shaken play music\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKCkz3n0gFj3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}