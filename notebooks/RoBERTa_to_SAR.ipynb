{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RoBERTa to SAR",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a767a8482d214325acedf5f032d8ee56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2bc9aed6fb1c40e3b8fd4128aaa35123",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8103c7f9a4f34b12b6911b810366c28d",
              "IPY_MODEL_f72bba9265e049c0829242d6038b2ee3"
            ]
          }
        },
        "2bc9aed6fb1c40e3b8fd4128aaa35123": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8103c7f9a4f34b12b6911b810366c28d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cdb3dc6e23944783b2e093b5dea29279",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 481,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 481,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_100c4bc3e10e41d89f761c17edaf3998"
          }
        },
        "f72bba9265e049c0829242d6038b2ee3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2323cf7eaf574105beeab443a20b321b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 481/481 [00:03&lt;00:00, 146B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cd764f26b6384eebaf47bcd0a7cae3d8"
          }
        },
        "cdb3dc6e23944783b2e093b5dea29279": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "100c4bc3e10e41d89f761c17edaf3998": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2323cf7eaf574105beeab443a20b321b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cd764f26b6384eebaf47bcd0a7cae3d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "35f04333013b49f79304fcdd9aa21d95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_75ccce8b60c14b0a86ad49de6b387736",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_610fa012a86148d3995c7391331f2dba",
              "IPY_MODEL_715ed1f3061743349ce2712fd62b0561"
            ]
          }
        },
        "75ccce8b60c14b0a86ad49de6b387736": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "610fa012a86148d3995c7391331f2dba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9e141d203de74d0e8fa0ecd244badf3a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898823,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898823,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c415902137a645ae96a6e55e68eb6a03"
          }
        },
        "715ed1f3061743349ce2712fd62b0561": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1330073b4c2a4369ace1a0955c705dfa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:00&lt;00:00, 2.69MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_709be53ca2224f7d95a39ce366d286ae"
          }
        },
        "9e141d203de74d0e8fa0ecd244badf3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c415902137a645ae96a6e55e68eb6a03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1330073b4c2a4369ace1a0955c705dfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "709be53ca2224f7d95a39ce366d286ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b80261f335f54dd0af64c1a24dbf9214": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3140dba81853486fb6d00a2e84066a61",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_39bd903c9c57491ca84ebc679966d195",
              "IPY_MODEL_1aca631a90004827aa7a277562eb640c"
            ]
          }
        },
        "3140dba81853486fb6d00a2e84066a61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "39bd903c9c57491ca84ebc679966d195": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2dd5d70fab2b4604843612fc55ae9c8a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9f56faa441f54477b61f93ab8e846ce0"
          }
        },
        "1aca631a90004827aa7a277562eb640c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_10d28656340a443e93110ae5e6f92651",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:01&lt;00:00, 241kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1923e8d5222940a09c085a9269dc0ef4"
          }
        },
        "2dd5d70fab2b4604843612fc55ae9c8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9f56faa441f54477b61f93ab8e846ce0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "10d28656340a443e93110ae5e6f92651": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1923e8d5222940a09c085a9269dc0ef4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5ac214e623d448e1bf415c48ad487479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_80f8154bcdd24ddea4b8144b8779937f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e3f7001297f74aaa93575baf9bc8a6ad",
              "IPY_MODEL_6ded1f0f29444b77a7d2d675dbfdaa70"
            ]
          }
        },
        "80f8154bcdd24ddea4b8144b8779937f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e3f7001297f74aaa93575baf9bc8a6ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2797f4a855aa429aa8bae91d0e73c3dc",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1355863,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1355863,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d4dc6e97547f4839916335d1eada2b4f"
          }
        },
        "6ded1f0f29444b77a7d2d675dbfdaa70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_42643aa6d72f43d39dffcfdc8e83b6e9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 4.17MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_66c98047d3264840bc1f3eea40e44105"
          }
        },
        "2797f4a855aa429aa8bae91d0e73c3dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d4dc6e97547f4839916335d1eada2b4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "42643aa6d72f43d39dffcfdc8e83b6e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "66c98047d3264840bc1f3eea40e44105": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "044e74ce67cd4493a0f0e19637184555": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ef3afbbac96948e2aef0fd962e8b6ce7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f1a51288dfbb48ee874e703863f0b58e",
              "IPY_MODEL_a3b5cb38833f4b858191c288aaf67827"
            ]
          }
        },
        "ef3afbbac96948e2aef0fd962e8b6ce7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f1a51288dfbb48ee874e703863f0b58e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2da2d56f9a814a069842d6f1358ea712",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 501200538,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 501200538,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_04932c3732ff485e82ebb132751b597f"
          }
        },
        "a3b5cb38833f4b858191c288aaf67827": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c0e410b1715f4ba28d80ae8fd87916d2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 501M/501M [01:02&lt;00:00, 7.98MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b34ae3e1f71e4748971f1d913cf49193"
          }
        },
        "2da2d56f9a814a069842d6f1358ea712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "04932c3732ff485e82ebb132751b597f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c0e410b1715f4ba28d80ae8fd87916d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b34ae3e1f71e4748971f1d913cf49193": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Masum06/Text2App/blob/master/notebooks/RoBERTa_to_SAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F864UIM1YUEq"
      },
      "source": [
        "Code borrowed from: https://github.com/microsoft/CodeXGLUE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nc12sfRZSmxT",
        "outputId": "05e708ee-de0c-44e1-95c6-33a27c2d2348"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jun 30 18:46:29 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBG3u_eYQHKY",
        "outputId": "35501144-ab3d-4a74-f1c9-5bc3fcbd46e7"
      },
      "source": [
        "!pip install transformers==4.5.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/91/61d69d58a1af1bd81d9ca9d62c90a6de3ab80d77f27c5df65d9a2c1f5626/transformers-4.5.0-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.2MB 11.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (4.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 55.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 42.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.0) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.0) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (3.0.4)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHNqpkh3Q2MG",
        "outputId": "6d90e25f-a6ae-42d4-f640-476650fb4bd3"
      },
      "source": [
        "!git clone https://github.com/Masum06/Text2App.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Text2App'...\n",
            "remote: Enumerating objects: 525, done.\u001b[K\n",
            "remote: Counting objects: 100% (128/128), done.\u001b[K\n",
            "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "remote: Total 525 (delta 79), reused 48 (delta 24), pack-reused 397\u001b[K\n",
            "Receiving objects: 100% (525/525), 233.22 MiB | 35.97 MiB/s, done.\n",
            "Resolving deltas: 100% (161/161), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRLNH4uJG7rI"
      },
      "source": [
        "# Text2App"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jd61C2XOiyd"
      },
      "source": [
        "Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "v6ziTZ3aTflj",
        "outputId": "24042bbd-e61c-461a-baa7-146ffbb6a506"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG59i7w5RCu_",
        "outputId": "c5ff0347-18c5-4b64-e364-3c142ed800fc"
      },
      "source": [
        "cd Text2App/training_RoBERTa/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Text2App/training_RoBERTa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx5HeX1IZK17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96e16188-3136-45f5-a666-a8075b49088e"
      },
      "source": [
        "# For saving checkpoint to Google Drive while working on Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yud9saHSVRi"
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv('../synthesized_data/nl_sar_train.csv')\n",
        "dev = pd.read_csv('../synthesized_data/nl_sar_valid.csv')\n",
        "test = pd.read_csv('../synthesized_data/nl_sar_test.csv')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0AwB1I6Yhle"
      },
      "source": [
        "class MyTokenizer:\n",
        "  vocab_size = 0\n",
        "  vocab = []\n",
        "  id_to_token = {}\n",
        "  token_to_id = {}\n",
        "\n",
        "  def __init__(self):\n",
        "    self.vocab = list(set(\" \".join(list(train['SAR'])).split()))\n",
        "    self.vocab.sort()\n",
        "    self.vocab_size = len(self.token_to_id) \n",
        "    # Special tokens: <s><pad></s><unk>\n",
        "    self.add_token('<s>')\n",
        "    self.add_token('<pad>')\n",
        "    self.add_token('</s>')\n",
        "    self.add_token('<unk>')\n",
        "    for v in self.vocab:\n",
        "      self.add_token(v)\n",
        "    self.add_token('None')\n",
        "\n",
        "  def tokenize(self, s):\n",
        "    return s.split()\n",
        "\n",
        "  def add_token(self, s):\n",
        "    if s not in self.token_to_id:\n",
        "      self.id_to_token[self.vocab_size] = s\n",
        "      self.token_to_id[s] = self.vocab_size\n",
        "      self.vocab_size+=1\n",
        "\n",
        "  def convert_string_to_ids(self, s):\n",
        "    tokens = s.split()\n",
        "    ids = []\n",
        "    for token in tokens:\n",
        "      ids.append(self.token_to_id[token])\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \"\"\n",
        "    for id in ids:\n",
        "      text += self.id_to_token[id] + \" \"\n",
        "    return text[:-1]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLOsuOtfPByQ"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VbExmvP0-Aw"
      },
      "source": [
        "# Copyright (c) Microsoft Corporation. \n",
        "# Licensed under the MIT license.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import copy\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "        Build Seqence-to-Sequence.\n",
        "        \n",
        "        Parameters:\n",
        "        * `encoder`- encoder of seq2seq model. e.g. roberta\n",
        "        * `decoder`- decoder of seq2seq model. e.g. transformer\n",
        "        * `config`- configuration of encoder model. \n",
        "        * `beam_size`- beam size for beam search. \n",
        "        * `max_length`- max length of target for beam search. \n",
        "        * `sos_id`- start of symbol ids in target for beam search.\n",
        "        * `eos_id`- end of symbol ids in target for beam search. \n",
        "    \"\"\"\n",
        "    def __init__(self, encoder,decoder,config,beam_size=None,max_length=None,sos_id=None,eos_id=None):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder=decoder\n",
        "        self.config=config\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(2048, 2048)))\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.lm_head = nn.Linear(config.hidden_size, decoder_tokenizer.vocab_size, bias=False) #config.vocab_size\n",
        "        self.lsm = nn.LogSoftmax(dim=-1)\n",
        "        # self.tie_weights()\n",
        "        \n",
        "        self.beam_size=beam_size\n",
        "        self.max_length=max_length\n",
        "        self.sos_id=sos_id\n",
        "        self.eos_id=eos_id\n",
        "        \n",
        "    def _tie_or_clone_weights(self, first_module, second_module):\n",
        "        \"\"\" Tie or clone module weights depending of weither we are using TorchScript or not\n",
        "        \"\"\"\n",
        "        if self.config.torchscript:\n",
        "            first_module.weight = nn.Parameter(second_module.weight.clone())\n",
        "        else:\n",
        "            first_module.weight = second_module.weight\n",
        "                  \n",
        "    def tie_weights(self):\n",
        "        \"\"\" Make sure we are sharing the input and output embeddings.\n",
        "            Export to TorchScript can't handle parameter sharing so we are cloning them instead.\n",
        "        \"\"\"\n",
        "        self._tie_or_clone_weights(self.lm_head,\n",
        "                                   self.encoder.embeddings.word_embeddings)        \n",
        "        \n",
        "    def forward(self, source_ids=None,source_mask=None,target_ids=None,target_mask=None,args=None):   \n",
        "        outputs = self.encoder(source_ids, attention_mask=source_mask)\n",
        "        encoder_output = outputs[0].permute([1,0,2]).contiguous()\n",
        "        if target_ids is not None:  \n",
        "            attn_mask=-1e4 *(1-self.bias[:target_ids.shape[1],:target_ids.shape[1]])\n",
        "            tgt_embeddings = self.encoder.embeddings(target_ids).permute([1,0,2]).contiguous() ## MH: Problmatic\n",
        "            out = self.decoder(tgt_embeddings,encoder_output,tgt_mask=attn_mask,memory_key_padding_mask=(1-source_mask).bool())\n",
        "            hidden_states = torch.tanh(self.dense(out)).permute([1,0,2]).contiguous()\n",
        "            lm_logits = self.lm_head(hidden_states)\n",
        "            # Shift so that tokens < n predict n\n",
        "            active_loss = target_mask[..., 1:].ne(0).view(-1) == 1\n",
        "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
        "            shift_labels = target_ids[..., 1:].contiguous()\n",
        "            # Flatten the tokens\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1))[active_loss],\n",
        "                            shift_labels.view(-1)[active_loss])\n",
        "\n",
        "            outputs = loss, loss*active_loss.sum(), active_loss.sum()\n",
        "            # print(\"Inside forward, outputs:\", outputs)\n",
        "            return outputs\n",
        "        else:\n",
        "            #Predict \n",
        "            preds=[]       \n",
        "            zero=torch.cuda.LongTensor(1).fill_(0)     \n",
        "            for i in range(source_ids.shape[0]):\n",
        "                context=encoder_output[:,i:i+1]\n",
        "                context_mask=source_mask[i:i+1,:]\n",
        "                beam = Beam(self.beam_size,self.sos_id,self.eos_id)\n",
        "                input_ids=beam.getCurrentState()\n",
        "                context=context.repeat(1, self.beam_size,1)\n",
        "                context_mask=context_mask.repeat(self.beam_size,1)\n",
        "                for _ in range(self.max_length): \n",
        "                    if beam.done():\n",
        "                        break\n",
        "                    attn_mask=-1e4 *(1-self.bias[:input_ids.shape[1],:input_ids.shape[1]])\n",
        "                    tgt_embeddings = self.encoder.embeddings(input_ids).permute([1,0,2]).contiguous()\n",
        "                    out = self.decoder(tgt_embeddings,context,tgt_mask=attn_mask,memory_key_padding_mask=(1-context_mask).bool())\n",
        "                    out = torch.tanh(self.dense(out))\n",
        "                    hidden_states=out.permute([1,0,2]).contiguous()[:,-1,:]\n",
        "                    out = self.lsm(self.lm_head(hidden_states)).data\n",
        "                    beam.advance(out)\n",
        "                    input_ids.data.copy_(input_ids.data.index_select(0, beam.getCurrentOrigin()))\n",
        "                    input_ids=torch.cat((input_ids,beam.getCurrentState()),-1)\n",
        "                hyp= beam.getHyp(beam.getFinal())\n",
        "                pred=beam.buildTargetTokens(hyp)[:self.beam_size]\n",
        "                pred=[torch.cat([x.view(-1) for x in p]+[zero]*(self.max_length-len(p))).view(1,-1) for p in pred]\n",
        "                preds.append(torch.cat(pred,0).unsqueeze(0))\n",
        "                \n",
        "            preds=torch.cat(preds,0)\n",
        "            # print(\"Inside Forward, preds: \", preds)\n",
        "            return preds   \n",
        "        \n",
        "        \n",
        "\n",
        "class Beam(object):\n",
        "    def __init__(self, size,sos,eos):\n",
        "        self.size = size\n",
        "        self.tt = torch.cuda\n",
        "        # The score for each translation on the beam.\n",
        "        self.scores = self.tt.FloatTensor(size).zero_()\n",
        "        # The backpointers at each time-step.\n",
        "        self.prevKs = []\n",
        "        # The outputs at each time-step.\n",
        "        self.nextYs = [self.tt.LongTensor(size)\n",
        "                       .fill_(0)]\n",
        "        self.nextYs[0][0] = sos\n",
        "        # Has EOS topped the beam yet.\n",
        "        self._eos = eos\n",
        "        self.eosTop = False\n",
        "        # Time and k pair for finished.\n",
        "        self.finished = []\n",
        "\n",
        "    def getCurrentState(self):\n",
        "        \"Get the outputs for the current timestep.\"\n",
        "        batch = self.tt.LongTensor(self.nextYs[-1]).view(-1, 1)\n",
        "        return batch\n",
        "\n",
        "    def getCurrentOrigin(self):\n",
        "        \"Get the backpointers for the current timestep.\"\n",
        "        return self.prevKs[-1]\n",
        "\n",
        "    def advance(self, wordLk):\n",
        "        \"\"\"\n",
        "        Given prob over words for every last beam `wordLk` and attention\n",
        "        `attnOut`: Compute and update the beam search.\n",
        "        Parameters:\n",
        "        * `wordLk`- probs of advancing from the last step (K x words)\n",
        "        * `attnOut`- attention at the last step\n",
        "        Returns: True if beam search is complete.\n",
        "        \"\"\"\n",
        "        numWords = wordLk.size(1)\n",
        "\n",
        "        # Sum the previous scores.\n",
        "        if len(self.prevKs) > 0:\n",
        "            beamLk = wordLk + self.scores.unsqueeze(1).expand_as(wordLk)\n",
        "\n",
        "            # Don't let EOS have children.\n",
        "            for i in range(self.nextYs[-1].size(0)):\n",
        "                if self.nextYs[-1][i] == self._eos:\n",
        "                    beamLk[i] = -1e20\n",
        "        else:\n",
        "            beamLk = wordLk[0]\n",
        "        flatBeamLk = beamLk.view(-1)\n",
        "        bestScores, bestScoresId = flatBeamLk.topk(self.size, 0, True, True)\n",
        "\n",
        "        self.scores = bestScores\n",
        "\n",
        "        # bestScoresId is flattened beam x word array, so calculate which\n",
        "        # word and beam each score came from\n",
        "        prevK = bestScoresId // numWords\n",
        "        self.prevKs.append(prevK)\n",
        "        self.nextYs.append((bestScoresId - prevK * numWords))\n",
        "\n",
        "\n",
        "        for i in range(self.nextYs[-1].size(0)):\n",
        "            if self.nextYs[-1][i] == self._eos:\n",
        "                s = self.scores[i]\n",
        "                self.finished.append((s, len(self.nextYs) - 1, i))\n",
        "\n",
        "        # End condition is when top-of-beam is EOS and no global score.\n",
        "        if self.nextYs[-1][0] == self._eos:\n",
        "            self.eosTop = True\n",
        "\n",
        "    def done(self):\n",
        "        return self.eosTop and len(self.finished) >=self.size\n",
        "\n",
        "    def getFinal(self):\n",
        "        if len(self.finished) == 0:\n",
        "            self.finished.append((self.scores[0], len(self.nextYs) - 1, 0))\n",
        "        self.finished.sort(key=lambda a: -a[0])\n",
        "        if len(self.finished) != self.size:\n",
        "            unfinished=[]\n",
        "            for i in range(self.nextYs[-1].size(0)):\n",
        "                if self.nextYs[-1][i] != self._eos:\n",
        "                    s = self.scores[i]\n",
        "                    unfinished.append((s, len(self.nextYs) - 1, i)) \n",
        "            unfinished.sort(key=lambda a: -a[0])\n",
        "            self.finished+=unfinished[:self.size-len(self.finished)]\n",
        "        return self.finished[:self.size]\n",
        "\n",
        "    def getHyp(self, beam_res):\n",
        "        \"\"\"\n",
        "        Walk back to construct the full hypothesis.\n",
        "        \"\"\"\n",
        "        hyps=[]\n",
        "        for _,timestep, k in beam_res:\n",
        "            hyp = []\n",
        "            for j in range(len(self.prevKs[:timestep]) - 1, -1, -1):\n",
        "                hyp.append(self.nextYs[j+1][k])\n",
        "                k = self.prevKs[j][k]\n",
        "            hyps.append(hyp[::-1])\n",
        "        return hyps\n",
        "    \n",
        "    def buildTargetTokens(self, preds):\n",
        "        sentence=[]\n",
        "        for pred in preds:\n",
        "            tokens = []\n",
        "            for tok in pred:\n",
        "                if tok==self._eos:\n",
        "                    break\n",
        "                tokens.append(tok)\n",
        "            sentence.append(tokens)\n",
        "        return sentence\n",
        "        \n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrH5YUx6IvWz"
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"\n",
        "Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\n",
        "GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\n",
        "using a masked language modeling (MLM) loss.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "import os\n",
        "import sys\n",
        "import bleu\n",
        "import pickle\n",
        "import torch\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "# import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from io import open\n",
        "from itertools import cycle\n",
        "import torch.nn as nn\n",
        "# from model import Seq2Seq\n",
        "from tqdm import tqdm, trange\n",
        "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
        "                          RobertaConfig, RobertaModel, RobertaTokenizer)\n",
        "MODEL_CLASSES = {'roberta': (RobertaConfig, RobertaModel, RobertaTokenizer)}\n",
        "\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class Example(object):\n",
        "    \"\"\"A single training/test example.\"\"\"\n",
        "    def __init__(self,\n",
        "                 idx,\n",
        "                 source,\n",
        "                 target,\n",
        "                 ):\n",
        "        self.idx = idx\n",
        "        self.source = source\n",
        "        self.target = target\n",
        "\n",
        "def read_examples(filename):\n",
        "    \"\"\"Read examples from filename.\"\"\"\n",
        "    examples=[]\n",
        "\n",
        "    ## open DF, convert to json, iterate one by one\n",
        "    # with open(filename,encoding=\"utf-8\") as f:\n",
        "    #     for idx, line in enumerate(f):\n",
        "    #         line=line.strip()\n",
        "    #         js=json.loads(line)\n",
        "    #         if 'idx' not in js:\n",
        "    #             js['idx']=idx\n",
        "    #         sar=' '.join(js['SAR']).replace('\\n',' ')\n",
        "    #         sar=' '.join(code.strip().split())\n",
        "    #         nl=' '.join(js['NL']).replace('\\n','')\n",
        "    #         nl=' '.join(nl.strip().split())  \n",
        "\n",
        "    df = pd.read_csv(filename)\n",
        "    data_list = list(df.T.to_dict().values())\n",
        "    for idx, data in enumerate(data_list):\n",
        "      nl = data['NL']\n",
        "      sar = data['SAR']\n",
        "      examples.append(\n",
        "          Example(\n",
        "                  idx = idx,\n",
        "                  source=nl,\n",
        "                  target=sar,\n",
        "                  ) \n",
        "      )\n",
        "    return examples\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single training/test features for a example.\"\"\"\n",
        "    def __init__(self,\n",
        "                 example_id,\n",
        "                 source_ids,\n",
        "                 target_ids,\n",
        "                 source_mask,\n",
        "                 target_mask,\n",
        "\n",
        "    ):\n",
        "        self.example_id = example_id\n",
        "        self.source_ids = source_ids\n",
        "        self.target_ids = target_ids\n",
        "        self.source_mask = source_mask\n",
        "        self.target_mask = target_mask       \n",
        "        \n",
        "\n",
        "\n",
        "def convert_examples_to_features(examples, tokenizer, args,stage=None): # MH: make it encoder_tokenizer, decoder_tokenizer\n",
        "    features = []\n",
        "    for example_index, example in enumerate(examples):\n",
        "        #source\n",
        "        source_tokens = tokenizer.tokenize(example.source)[:args.max_source_length-2]\n",
        "        source_tokens =[tokenizer.cls_token]+source_tokens+[tokenizer.sep_token]\n",
        "        source_ids =  tokenizer.convert_tokens_to_ids(source_tokens) \n",
        "        source_mask = [1] * (len(source_tokens))\n",
        "        padding_length = args.max_source_length - len(source_ids)\n",
        "        source_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "        source_mask+=[0]*padding_length\n",
        "\n",
        "        #target\n",
        "        if stage==\"test\":\n",
        "            target_tokens = ['None'] #tokenizer.tokenize(\"None\")\n",
        "        else:\n",
        "            # target_tokens = tokenizer.tokenize(example.target)[:args.max_target_length-2]\n",
        "            target_tokens = example.target.split()[:args.max_target_length-2]\n",
        "        target_tokens = [tokenizer.cls_token]+target_tokens+[tokenizer.sep_token]            \n",
        "        # target_ids = tokenizer.convert_tokens_to_ids(target_tokens) # MH: decoder\n",
        "        target_ids = decoder_tokenizer.convert_string_to_ids(' '.join(target_tokens))\n",
        "        target_mask = [1] *len(target_ids)\n",
        "        padding_length = args.max_target_length - len(target_ids)\n",
        "        target_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "        target_mask+=[0]*padding_length   \n",
        "\n",
        "        if example_index < 5:\n",
        "            if stage=='train':\n",
        "                print(\"*** Example ***\")\n",
        "                print(\"idx: {}\".format(example.idx))\n",
        "\n",
        "                print(\"source_tokens: {}\".format([x.replace('\\u0120','_') for x in source_tokens]))\n",
        "                print(\"source_ids: {}\".format(' '.join(map(str, source_ids))))\n",
        "                print(\"source_mask: {}\".format(' '.join(map(str, source_mask))))\n",
        "                \n",
        "                print(\"target_tokens: {}\".format([x.replace('\\u0120','_') for x in target_tokens]))\n",
        "                print(\"target_ids: {}\".format(' '.join(map(str, target_ids))))\n",
        "                print(\"target_mask: {}\".format(' '.join(map(str, target_mask))))\n",
        "       \n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                 example_index,\n",
        "                 source_ids,\n",
        "                 target_ids,\n",
        "                 source_mask,\n",
        "                 target_mask,\n",
        "            )\n",
        "        )\n",
        "    return features\n",
        "\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYHTONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    \n",
        "\n",
        "def main():\n",
        "\n",
        "    print(args)\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(args.seed)\n",
        "    # make dir if output_dir not exist\n",
        "    if os.path.exists(args.output_dir) is False:\n",
        "        os.makedirs(args.output_dir)\n",
        "        \n",
        "## model was here\n",
        "\n",
        "    if args.do_train:\n",
        "        print(\"Inside TRAIN\")\n",
        "        # Prepare training data loader\n",
        "        train_examples = read_examples(args.train_filename)\n",
        "        train_features = convert_examples_to_features(train_examples, tokenizer,args,stage='train') # MH: 2 tokenizers\n",
        "        all_source_ids = torch.tensor([f.source_ids for f in train_features], dtype=torch.long)\n",
        "        all_source_mask = torch.tensor([f.source_mask for f in train_features], dtype=torch.long)\n",
        "        all_target_ids = torch.tensor([f.target_ids for f in train_features], dtype=torch.long)\n",
        "        all_target_mask = torch.tensor([f.target_mask for f in train_features], dtype=torch.long)    \n",
        "        train_data = TensorDataset(all_source_ids,all_source_mask,all_target_ids,all_target_mask)\n",
        "        \n",
        "        if args.local_rank == -1:\n",
        "            train_sampler = RandomSampler(train_data)\n",
        "        else:\n",
        "            train_sampler = DistributedSampler(train_data)\n",
        "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size//args.gradient_accumulation_steps)\n",
        "\n",
        "        num_train_optimization_steps =  args.train_steps\n",
        "\n",
        "        # Prepare optimizer and schedule (linear warmup and decay)\n",
        "        no_decay = ['bias', 'LayerNorm.weight']\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay': args.weight_decay},\n",
        "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                    num_warmup_steps=int(t_total*0.1),\n",
        "                                                    num_training_steps=t_total)\n",
        "    \n",
        "        #Start training\n",
        "        print(\"***** Running training *****\")\n",
        "        print(\"  Num examples = %d\", len(train_examples))\n",
        "        print(\"  Batch size = %d\", args.train_batch_size)\n",
        "        print(\"  Num epoch = %d\", args.num_train_epochs)\n",
        "        \n",
        "\n",
        "        model.train()\n",
        "        dev_dataset={}\n",
        "        nb_tr_examples, nb_tr_steps,tr_loss,global_step,best_bleu,best_loss = 0, 0,0,0,0,1e6 \n",
        "        for epoch in range(args.num_train_epochs):\n",
        "            bar = tqdm(train_dataloader,total=len(train_dataloader))\n",
        "            for batch in bar:\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                source_ids,source_mask,target_ids,target_mask = batch\n",
        "                loss,_,_ = model(source_ids=source_ids,source_mask=source_mask,target_ids=target_ids,target_mask=target_mask)\n",
        "\n",
        "                if args.n_gpu > 1:\n",
        "                    loss = loss.mean() # mean() to average on multi-gpu.\n",
        "                if args.gradient_accumulation_steps > 1:\n",
        "                    loss = loss / args.gradient_accumulation_steps\n",
        "                tr_loss += loss.item()\n",
        "                train_loss=round(tr_loss*args.gradient_accumulation_steps/(nb_tr_steps+1),4)\n",
        "                bar.set_description(\"epoch {} loss {}\".format(epoch,train_loss))\n",
        "                nb_tr_examples += source_ids.size(0)\n",
        "                nb_tr_steps += 1\n",
        "                loss.backward()\n",
        "\n",
        "                if (nb_tr_steps + 1) % args.gradient_accumulation_steps == 0:\n",
        "                    #Update parameters\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "                    scheduler.step()\n",
        "                    global_step += 1\n",
        "\n",
        "            if args.do_eval:\n",
        "                print(\"Inside EVAL\")\n",
        "                #Eval model with dev dataset\n",
        "                tr_loss = 0\n",
        "                nb_tr_examples, nb_tr_steps = 0, 0                     \n",
        "                eval_flag=False    \n",
        "                if 'dev_loss' in dev_dataset:\n",
        "                    eval_examples,eval_data=dev_dataset['dev_loss']\n",
        "                else:\n",
        "                    eval_examples = read_examples(args.dev_filename)\n",
        "                    eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='dev')\n",
        "                    all_source_ids = torch.tensor([f.source_ids for f in eval_features], dtype=torch.long)\n",
        "                    all_source_mask = torch.tensor([f.source_mask for f in eval_features], dtype=torch.long)\n",
        "                    all_target_ids = torch.tensor([f.target_ids for f in eval_features], dtype=torch.long)\n",
        "                    all_target_mask = torch.tensor([f.target_mask for f in eval_features], dtype=torch.long)      \n",
        "                    eval_data = TensorDataset(all_source_ids,all_source_mask,all_target_ids,all_target_mask)   \n",
        "                    dev_dataset['dev_loss']=eval_examples,eval_data\n",
        "                eval_sampler = SequentialSampler(eval_data)\n",
        "                eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "                print(\"\\n***** Running evaluation *****\")\n",
        "                print(\"  Num examples = %d\", len(eval_examples))\n",
        "                print(\"  Batch size = %d\", args.eval_batch_size)\n",
        "\n",
        "                #Start Evaling model\n",
        "                model.eval()\n",
        "                eval_loss,tokens_num = 0,0\n",
        "                for batch in eval_dataloader:\n",
        "                    batch = tuple(t.to(device) for t in batch)\n",
        "                    source_ids,source_mask,target_ids,target_mask = batch                  \n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        _,loss,num = model(source_ids=source_ids,source_mask=source_mask,\n",
        "                                           target_ids=target_ids,target_mask=target_mask)     \n",
        "                    eval_loss += loss.sum().item()\n",
        "                    tokens_num += num.sum().item()\n",
        "                #Pring loss of dev dataset    \n",
        "                model.train()\n",
        "                eval_loss = eval_loss / tokens_num\n",
        "                result = {'eval_ppl': round(np.exp(eval_loss),5),\n",
        "                          'global_step': global_step+1,\n",
        "                          'train_loss': round(train_loss,5)}\n",
        "                for key in sorted(result.keys()):\n",
        "                    print(\"  %s = %s\", key, str(result[key]))\n",
        "                print(\"  \"+\"*\"*20)   \n",
        "\n",
        "                #save last checkpoint\n",
        "                last_output_dir = os.path.join(args.output_dir, 'checkpoint-last')\n",
        "                if not os.path.exists(last_output_dir):\n",
        "                    os.makedirs(last_output_dir)\n",
        "                model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "                output_model_file = os.path.join(last_output_dir, \"pytorch_model.bin\")\n",
        "                torch.save(model_to_save.state_dict(), output_model_file)                    \n",
        "                if eval_loss<best_loss:\n",
        "                    print(\"  Best ppl:%s\",round(np.exp(eval_loss),5))\n",
        "                    print(\"  \"+\"*\"*20)\n",
        "                    best_loss=eval_loss\n",
        "                    # Save best checkpoint for best ppl\n",
        "                    output_dir = os.path.join(args.output_dir, 'checkpoint-best-ppl')\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "                    output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
        "                    torch.save(model_to_save.state_dict(), output_model_file)  \n",
        "\n",
        "\n",
        "                #Calculate bleu  \n",
        "                print(\"Calculating BLEU\")\n",
        "                if 'dev_bleu' in dev_dataset:\n",
        "                    eval_examples,eval_data=dev_dataset['dev_bleu']\n",
        "                else:\n",
        "                    eval_examples = read_examples(args.dev_filename)\n",
        "                    eval_examples = random.sample(eval_examples,min(1000,len(eval_examples)))\n",
        "                    eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='test')\n",
        "                    all_source_ids = torch.tensor([f.source_ids for f in eval_features], dtype=torch.long)\n",
        "                    all_source_mask = torch.tensor([f.source_mask for f in eval_features], dtype=torch.long)    \n",
        "                    eval_data = TensorDataset(all_source_ids,all_source_mask)   \n",
        "                    dev_dataset['dev_bleu']=eval_examples,eval_data\n",
        "\n",
        "\n",
        "\n",
        "                eval_sampler = SequentialSampler(eval_data)\n",
        "                eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "                model.eval() \n",
        "                p=[]\n",
        "                for batch in eval_dataloader:\n",
        "                    batch = tuple(t.to(device) for t in batch)\n",
        "                    source_ids,source_mask= batch                  \n",
        "                    with torch.no_grad():\n",
        "                        preds = model(source_ids=source_ids,source_mask=source_mask)  \n",
        "                        for pred in preds:\n",
        "                            t=pred[0].cpu().numpy()\n",
        "                            t=list(t)\n",
        "                            if 0 in t:\n",
        "                                t=t[:t.index(0)]\n",
        "                            # text = tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
        "                            text = decoder_tokenizer.decode(t) # MH: decoder\n",
        "                            p.append(text)\n",
        "                model.train()\n",
        "                predictions=[]\n",
        "                with open(os.path.join(args.output_dir,\"dev.output\"),'w') as f, open(os.path.join(args.output_dir,\"dev.gold\"),'w') as f1:\n",
        "                    for ref,gold in zip(p,eval_examples):\n",
        "                        predictions.append(str(gold.idx)+'\\t'+ref)\n",
        "                        f.write(str(gold.idx)+'\\t'+ref+'\\n')\n",
        "                        f1.write(str(gold.idx)+'\\t'+gold.target+'\\n')     \n",
        "\n",
        "                (goldMap, predictionMap) = bleu.computeMaps(predictions, os.path.join(args.output_dir, \"dev.gold\")) \n",
        "                dev_bleu=round(bleu.bleuFromMaps(goldMap, predictionMap)[0],2)\n",
        "                print(\"  %s = %s \"%(\"bleu-4\",str(dev_bleu)))\n",
        "                print(\"  \"+\"*\"*20)    \n",
        "                if dev_bleu>best_bleu:\n",
        "                    print(\"  Best bleu:%s\",dev_bleu)\n",
        "                    print(\"  \"+\"*\"*20)\n",
        "                    best_bleu=dev_bleu\n",
        "                    # Save best checkpoint for best bleu\n",
        "                    output_dir = os.path.join(args.output_dir, 'checkpoint-best-bleu')\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "                    output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
        "                    torch.save(model_to_save.state_dict(), output_model_file)\n",
        "               \n",
        "    if args.do_test:\n",
        "        # print(\"Inside TEST\")\n",
        "        # files=[]\n",
        "        # if args.dev_filename is not None:\n",
        "        #     files.append(args.dev_filename)\n",
        "        # if args.test_filename is not None:\n",
        "        #     files.append(args.test_filename)\n",
        "        # for idx,file in enumerate(files):   \n",
        "        idx = 0\n",
        "        file = args.test_filename # Change it to test file later\n",
        "        print(\"Test file: {}\".format(file))\n",
        "        eval_examples = read_examples(file)\n",
        "        eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='test')\n",
        "        all_source_ids = torch.tensor([f.source_ids for f in eval_features], dtype=torch.long)\n",
        "        all_source_mask = torch.tensor([f.source_mask for f in eval_features], dtype=torch.long)    \n",
        "        eval_data = TensorDataset(all_source_ids,all_source_mask)   \n",
        "\n",
        "        # Calculate bleu\n",
        "        eval_sampler = SequentialSampler(eval_data)\n",
        "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "        model.eval() \n",
        "        p=[]\n",
        "        for batch in tqdm(eval_dataloader,total=len(eval_dataloader)):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            source_ids,source_mask= batch                  \n",
        "            with torch.no_grad():\n",
        "                preds = model(source_ids=source_ids,source_mask=source_mask) \n",
        "                for pred in preds:\n",
        "                    t=pred[0].cpu().numpy()\n",
        "                    t=list(t)\n",
        "                    if 0 in t:\n",
        "                        t=t[:t.index(0)]\n",
        "                    # text = tokenizer.decode(t,clean_up_tokenization_spaces=False) # MH: decoder\n",
        "                    text = decoder_tokenizer.decode(t)\n",
        "                    p.append(text)\n",
        "        model.train()\n",
        "        predictions=[]\n",
        "        with open(os.path.join(args.output_dir,\"test_{}.output\".format(str(idx))),'w') as f, open(os.path.join(args.output_dir,\"test_{}.gold\".format(str(idx))),'w') as f1:\n",
        "            for ref,gold in zip(p,eval_examples):\n",
        "                predictions.append(str(gold.idx)+'\\t'+ref)\n",
        "                f.write(str(gold.idx)+'\\t'+ref+'\\n')\n",
        "                f1.write(str(gold.idx)+'\\t'+gold.target+'\\n')     \n",
        "\n",
        "        (goldMap, predictionMap) = bleu.computeMaps(predictions, os.path.join(args.output_dir, args.output_name+\"test_{}.gold\".format(idx))) \n",
        "        dev_bleu=round(bleu.bleuFromMaps(goldMap, predictionMap)[0],2)\n",
        "        print(\"  %s = %s \"%(\"bleu-4\",str(dev_bleu)))\n",
        "        print(\"  \"+\"*\"*20)    \n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffH7YvV6Oaxz"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34alizypPOH9"
      },
      "source": [
        "class Arguments:\n",
        "    pass"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAaxEIuUM2uG"
      },
      "source": [
        "output_dir=\"/gdrive/My Drive/text2app_models/RoBERTa/\" # Colab + Drive\n",
        "# output_dir=\"model/\" # Local\n",
        "data_dir = '../synthesized_data/'\n",
        "train_file=data_dir+'nl_sar_train.csv'\n",
        "dev_file=data_dir+'nl_sar_valid.csv'\n",
        "test_file=data_dir+'nl_sar_test.csv'\n",
        "pretrained_model= 'roberta-base' #'microsoft/codebert-base' #'roberta-base'"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpOUBhufLt-w"
      },
      "source": [
        "args = Arguments()\n",
        "\n",
        "## Required parameters\n",
        "args.model_type='roberta'\n",
        "args.model_name_or_path=pretrained_model\n",
        "args.output_dir=output_dir\n",
        "args.load_model_path=None #output_dir+\"/checkpoint-best-bleu/pytorch_model.bin\"\n",
        "## Other parameters\n",
        "args.train_filename=train_file\n",
        "args.dev_filename=dev_file\n",
        "args.test_filename=test_file\n",
        "args.output_name = \"\"\n",
        "args.config_name=\"\"\n",
        "args.tokenizer_name=\"\"\n",
        "args.gradient_accumulation_steps=1\n",
        "args.weight_decay=0.0\n",
        "args.adam_epsilon=1e-8\n",
        "args.max_grad_norm=1.0\n",
        "args.max_steps=-1\n",
        "args.eval_steps=-1\n",
        "args.train_steps=-1\n",
        "args.warmup_steps=0\n",
        "args.local_rank=-1\n",
        "args.seed=42\n",
        "\n",
        "args.no_cuda=False\n",
        "args.do_lower_case=True\n",
        "args.do_train=True\n",
        "args.do_eval=True\n",
        "args.do_test=False\n",
        "\n",
        "args.num_train_epochs=10\n",
        "args.train_batch_size=100\n",
        "args.eval_batch_size=100\n",
        "args.learning_rate=5e-5\n",
        "args.max_source_length=80\n",
        "args.max_target_length=50\n",
        "args.beam_size=5"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSceeljfzY0I",
        "outputId": "9697d6ad-8883-4371-b691-f6b03a27e844"
      },
      "source": [
        "# Setup CUDA, GPU & distributed training\n",
        "if args.local_rank == -1 or args.no_cuda:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "    args.n_gpu = torch.cuda.device_count()\n",
        "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "    torch.cuda.set_device(args.local_rank)\n",
        "    device = torch.device(\"cuda\", args.local_rank)\n",
        "    torch.distributed.init_process_group(backend='nccl')\n",
        "    args.n_gpu = 1\n",
        "logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s\",\n",
        "                args.local_rank, device, args.n_gpu, bool(args.local_rank != -1))\n",
        "args.device = device"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "06/30/2021 18:49:31 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YVnN-E_zEcj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461,
          "referenced_widgets": [
            "a767a8482d214325acedf5f032d8ee56",
            "2bc9aed6fb1c40e3b8fd4128aaa35123",
            "8103c7f9a4f34b12b6911b810366c28d",
            "f72bba9265e049c0829242d6038b2ee3",
            "cdb3dc6e23944783b2e093b5dea29279",
            "100c4bc3e10e41d89f761c17edaf3998",
            "2323cf7eaf574105beeab443a20b321b",
            "cd764f26b6384eebaf47bcd0a7cae3d8",
            "35f04333013b49f79304fcdd9aa21d95",
            "75ccce8b60c14b0a86ad49de6b387736",
            "610fa012a86148d3995c7391331f2dba",
            "715ed1f3061743349ce2712fd62b0561",
            "9e141d203de74d0e8fa0ecd244badf3a",
            "c415902137a645ae96a6e55e68eb6a03",
            "1330073b4c2a4369ace1a0955c705dfa",
            "709be53ca2224f7d95a39ce366d286ae",
            "b80261f335f54dd0af64c1a24dbf9214",
            "3140dba81853486fb6d00a2e84066a61",
            "39bd903c9c57491ca84ebc679966d195",
            "1aca631a90004827aa7a277562eb640c",
            "2dd5d70fab2b4604843612fc55ae9c8a",
            "9f56faa441f54477b61f93ab8e846ce0",
            "10d28656340a443e93110ae5e6f92651",
            "1923e8d5222940a09c085a9269dc0ef4",
            "5ac214e623d448e1bf415c48ad487479",
            "80f8154bcdd24ddea4b8144b8779937f",
            "e3f7001297f74aaa93575baf9bc8a6ad",
            "6ded1f0f29444b77a7d2d675dbfdaa70",
            "2797f4a855aa429aa8bae91d0e73c3dc",
            "d4dc6e97547f4839916335d1eada2b4f",
            "42643aa6d72f43d39dffcfdc8e83b6e9",
            "66c98047d3264840bc1f3eea40e44105",
            "044e74ce67cd4493a0f0e19637184555",
            "ef3afbbac96948e2aef0fd962e8b6ce7",
            "f1a51288dfbb48ee874e703863f0b58e",
            "a3b5cb38833f4b858191c288aaf67827",
            "2da2d56f9a814a069842d6f1358ea712",
            "04932c3732ff485e82ebb132751b597f",
            "c0e410b1715f4ba28d80ae8fd87916d2",
            "b34ae3e1f71e4748971f1d913cf49193"
          ]
        },
        "outputId": "b7902752-5e24-4e66-9f3b-70a67cc7959c"
      },
      "source": [
        "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
        "config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path)\n",
        "tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,do_lower_case=args.do_lower_case)\n",
        "\n",
        "decoder_tokenizer = MyTokenizer()\n",
        "\n",
        "#budild model\n",
        "encoder = model_class.from_pretrained(args.model_name_or_path,config=config)    \n",
        "decoder_layer = nn.TransformerDecoderLayer(d_model=config.hidden_size, nhead=config.num_attention_heads)\n",
        "decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "06/30/2021 18:49:35 - INFO - filelock -   Lock 140237806276560 acquired on /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a767a8482d214325acedf5f032d8ee56",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=481.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/30/2021 18:49:35 - INFO - filelock -   Lock 140237806276560 released on /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "06/30/2021 18:49:35 - INFO - filelock -   Lock 140235175927760 acquired on /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35f04333013b49f79304fcdd9aa21d95",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/30/2021 18:49:36 - INFO - filelock -   Lock 140235175927760 released on /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "06/30/2021 18:49:36 - INFO - filelock -   Lock 140237939571600 acquired on /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b80261f335f54dd0af64c1a24dbf9214",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/30/2021 18:49:37 - INFO - filelock -   Lock 140237939571600 released on /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "06/30/2021 18:49:38 - INFO - filelock -   Lock 140235176148752 acquired on /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ac214e623d448e1bf415c48ad487479",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355863.0, style=ProgressStyle(descriptâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/30/2021 18:49:38 - INFO - filelock -   Lock 140235176148752 released on /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "06/30/2021 18:49:39 - INFO - filelock -   Lock 140235014615952 acquired on /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "044e74ce67cd4493a0f0e19637184555",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=501200538.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/30/2021 18:49:49 - INFO - filelock -   Lock 140235014615952 released on /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_SNN0D9EB8p"
      },
      "source": [
        "model=Seq2Seq(encoder=encoder,decoder=decoder,config=config,\n",
        "              beam_size=args.beam_size,max_length=args.max_target_length,\n",
        "              sos_id=tokenizer.cls_token_id,eos_id=tokenizer.sep_token_id)\n",
        "if args.load_model_path is not None:\n",
        "    print(\"reload model from {}\".format(args.load_model_path))\n",
        "    model.load_state_dict(torch.load(args.load_model_path))\n",
        "    \n",
        "model.to(device)\n",
        "if args.local_rank != -1:\n",
        "    # Distributed training\n",
        "    try:\n",
        "        from apex.parallel import DistributedDataParallel as DDP\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
        "    model = DDP(model)\n",
        "elif args.n_gpu > 1:\n",
        "    # multi-gpu training\n",
        "    model = torch.nn.DataParallel(model)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8fS_vh0I0P5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35fa582c-652b-4d55-a920-ec7a24736413"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.Arguments object at 0x7f8b4f4cce90>\n",
            "Inside TRAIN\n",
            "*** Example ***\n",
            "idx: 0\n",
            "source_tokens: ['<s>', 'make', '_software', '_consisting', '_of', '_a', '_ball', '_,', '_a', '_switch', '_with', '_text', '_string', '0', '_,', '_and', '_a', '_pas', 'word', '_text', '_box', '_.', '_when', '_the', '_ball', '_is', '_reaches', '_edge', ',', '_set', '_ball', '_in', '_motion', '_.', '</s>']\n",
            "source_ids: 0 19746 2257 17402 9 10 1011 2156 10 5405 19 2788 6755 288 2156 8 10 6977 14742 2788 2233 479 77 5 1011 16 11541 3543 6 278 1011 11 4298 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "target_tokens: ['<s>', '<complist>', '<ball>', '<switch>', 'string0', '</switch>', '<passwordtextbox>', '</complist>', '<code>', '<ball1flung>', '<ball1>', '<motion>', '</ball1>', '</ball1flung>', '</code>', '</s>']\n",
            "target_ids: 0 50 40 78 103 29 64 14 48 38 37 62 5 6 12 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "*** Example ***\n",
            "idx: 1\n",
            "source_tokens: ['<s>', 'mobile', '_application', '_that', '_has', '_a', '_switch', '_,', '_a', '_circle', '_,', '_a', '_password', '_field', '_,', '_a', '_password', '_box', '_,', '_a', '_switch', '_,', '_an', '_acceler', 'ometer', '_,', '_and', '_a', '_video', '_player', '_with', '_source', '_string', '0', '_.', '_when', '_the', '_initial', '_switch', '_is', '_pressed', ',', '_set', '_the', '_radius', '_to', '_number', '0', '_.', '_when', '_the', '_ball', '_is', '_touches', '_edge', ',', '_pause', '_video', '_player', '_.', '</s>']\n",
            "source_ids: 0 25254 2502 14 34 10 5405 2156 10 7922 2156 10 14844 882 2156 10 14844 2233 2156 10 5405 2156 41 27416 12687 2156 8 10 569 869 19 1300 6755 288 479 77 5 2557 5405 16 11224 6 278 5 29943 7 346 288 479 77 5 1011 16 12325 3543 6 13787 569 869 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "target_tokens: ['<s>', '<complist>', '<switch>', 'switch1', '</switch>', '<ball>', '<passwordtextbox>', '<passwordtextbox>', '<switch>', 'switch2', '</switch>', '<accelerometer>', '<video_player>', 'string0', '</video_player>', '</complist>', '<code>', '<switch1flipped>', '<ball1>', '<radius>', 'number0', '</radius>', '</ball1>', '</switch1flipped>', '<ball1reach_edge>', '<video_player1>', '<stop>', '</video_player1>', '</ball1reach_edge>', '</code>', '</s>']\n",
            "target_ids: 0 50 78 111 29 40 64 64 78 112 29 36 92 103 34 14 48 75 37 70 100 24 5 26 39 89 74 31 7 12 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "*** Example ***\n",
            "idx: 2\n",
            "source_tokens: ['<s>', 'an', '_app', '_consisting', '_of', '_a', '_text', '2', 'speech', '_,', '_a', '_video', '_with', '_source', '_string', '0', '_,', '_a', '_circle', '_,', '_a', '_video', '_with', '_source', '_string', '1', '_,', '_a', '_switch', '_named', '_string', '2', '_,', '_a', '_label', '_,', '_and', '_a', '_video', '_with', '_source', '_string', '3', '_.', '_if', '_the', '_circle', '_is', '_thrown', ',', '_set', '_label', '_text', '_to', '_string', '4', '_.', '_when', '_the', '_switch', '_is', '_flip', 'ed', ',', '_play', '_the', '_video', '_.', '</s>']\n",
            "source_ids: 0 260 1553 17402 9 10 2788 176 40511 2156 10 569 19 1300 6755 288 2156 10 7922 2156 10 569 19 1300 6755 134 2156 10 5405 1440 6755 176 2156 10 6929 2156 8 10 569 19 1300 6755 246 479 114 5 7922 16 5629 6 278 6929 2788 7 6755 306 479 77 5 5405 16 11113 196 6 310 5 569 479 2 1 1 1 1 1 1 1 1 1 1 1\n",
            "source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
            "target_tokens: ['<s>', '<complist>', '<text2speech>', '<video_player>', 'string0', '</video_player>', '<ball>', '<video_player>', 'string1', '</video_player>', '<switch>', 'string2', '</switch>', '<label>', 'label1', '</label>', '<video_player>', 'string3', '</video_player>', '</complist>', '<code>', '<ball1reach_edge>', '<label1>', 'string4', '</label1>', '</ball1reach_edge>', '<switch1flipped>', '<video_player1>', '<start>', '</video_player1>', '</switch1flipped>', '</code>', '</s>']\n",
            "target_ids: 0 50 80 92 103 34 40 92 104 34 78 105 29 59 97 19 92 106 34 14 48 39 56 107 16 7 75 89 73 31 26 12 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "*** Example ***\n",
            "idx: 3\n",
            "source_tokens: ['<s>', 'create', '_software', '_with', '_a', '_video', '_player', '_with', '_source', '_string', '0', '_,', '_and', '_an', '_audio', '_with', '_source', '_string', '1', '_.', '</s>']\n",
            "source_ids: 0 32845 2257 19 10 569 869 19 1300 6755 288 2156 8 41 6086 19 1300 6755 134 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "target_tokens: ['<s>', '<complist>', '<video_player>', 'string0', '</video_player>', '<player>', 'string1', '</player>', '</complist>', '</s>']\n",
            "target_ids: 0 50 92 103 34 69 104 23 14 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "target_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "*** Example ***\n",
            "idx: 4\n",
            "source_tokens: ['<s>', 'make', '_mobile', '_application', '_containing', '_a', '_video', '_player', '_with', '_source', '_string', '0', '_.', '</s>']\n",
            "source_ids: 0 19746 1830 2502 8200 10 569 869 19 1300 6755 288 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "target_tokens: ['<s>', '<complist>', '<video_player>', 'string0', '</video_player>', '</complist>', '</s>']\n",
            "target_ids: 0 50 92 103 34 14 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "target_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/400 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = %d 40000\n",
            "  Batch size = %d 100\n",
            "  Num epoch = %d 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0 loss 1.9014: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [12:08<00:00,  1.82s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inside EVAL\n",
            "\n",
            "***** Running evaluation *****\n",
            "  Num examples = %d 5000\n",
            "  Batch size = %d 100\n",
            "  %s = %s eval_ppl 1.28222\n",
            "  %s = %s global_step 401\n",
            "  %s = %s train_loss 1.9014\n",
            "  ********************\n",
            "  Best ppl:%s 1.28222\n",
            "  ********************\n",
            "Calculating BLEU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            "Total: 1000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  bleu-4 = 92.43 \n",
            "  ********************\n",
            "  Best bleu:%s 92.43\n",
            "  ********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1 loss 0.1162: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [12:21<00:00,  1.85s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inside EVAL\n",
            "\n",
            "***** Running evaluation *****\n",
            "  Num examples = %d 5000\n",
            "  Batch size = %d 100\n",
            "  %s = %s eval_ppl 1.0467\n",
            "  %s = %s global_step 801\n",
            "  %s = %s train_loss 0.1162\n",
            "  ********************\n",
            "  Best ppl:%s 1.0467\n",
            "  ********************\n",
            "Calculating BLEU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TptSjxB5Ye96"
      },
      "source": [
        "# If working on colab, save checkpoint to Google Drive\n",
        "!cp model/checkpoint-best-bleu/pytorch_model.bin '/gdrive/My Drive/text2app_models/RoBERTa/' # CodeBERT RoBERTa PointerNet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIV6rQ0oOUPd"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-V6r9YtBOPWG"
      },
      "source": [
        "args.do_test=True\n",
        "args.do_train=False\n",
        "args.beam_size=1\n",
        "args.test_filename=data_dir+'nl_sar_test_unseen_pair.csv'\n",
        "args.output_name = \"unseen_pair_\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LDVxq3eJ4Xf"
      },
      "source": [
        "**Test Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5Do7psBVMLW"
      },
      "source": [
        "model=Seq2Seq(encoder=encoder,decoder=decoder,config=config,\n",
        "              beam_size=args.beam_size,max_length=args.max_target_length,\n",
        "              sos_id=tokenizer.cls_token_id,eos_id=tokenizer.sep_token_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUa9ag27Dyms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b81091c5-73dd-4f97-eec7-fc5cf6a1c859"
      },
      "source": [
        "# args.load_model_path=output_dir+\"/checkpoint-best-bleu/pytorch_model.bin\"\n",
        "args.load_model_path='/gdrive/My Drive/text2app_models/RoBERTa/pytorch_model.bin'\n",
        "\n",
        "model.load_state_dict(torch.load(args.load_model_path))\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (decoder): TransformerDecoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (3): TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (4): TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (5): TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (lm_head): Linear(in_features=768, out_features=115, bias=False)\n",
              "  (lsm): LogSoftmax(dim=-1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x15hh5QgBweO"
      },
      "source": [
        "for test_file in ['nl_sar_test.csv', 'nl_sar_test_2%_mutation.csv', 'nl_sar_test_5%_mutation.csv', 'nl_sar_test_10%_mutation.csv', 'nl_sar_test_unseen_pair.csv']:\n",
        "  args.test_filename=data_dir+test_file\n",
        "  args.output_name=test_file[10:-4]\n",
        "  print(\"#### testing\", test_file)\n",
        "\n",
        "  if __name__ == \"__main__\":\n",
        "      main()\n",
        "\n",
        "  ref = open(\"model/test_0.gold\")\n",
        "  ref_sar = ref.readlines()\n",
        "  ref.close()\n",
        "  pred = open(\"model/\"+args.output_name+\"test_0.output\")\n",
        "  pred_sar = pred.readlines()\n",
        "  pred.close()\n",
        "  correct = 0\n",
        "  for i in range(len(ref_sar)):\n",
        "    if ref_sar[i]==pred_sar[i]:\n",
        "      correct+=1\n",
        "  print(\"Exact Match: \", 100*correct/len(ref_sar))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVNsWW7HQNvs"
      },
      "source": [
        "## Calculate BLEU-4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNOFrVjDQQbr",
        "outputId": "d0edbe83-a3e1-4d0b-84ed-04cf0d400629"
      },
      "source": [
        "!python evaluator.py model/test_0.gold < model/test_0.output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total: 5000\n",
            "97.20048209170629\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfVbwSg4bnpx"
      },
      "source": [
        "## Exact Match"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paPK1mbrRtmF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2678aca8-8ec5-4d29-834f-cec7e3cafd8b"
      },
      "source": [
        "ref = open(\"model/test_0.gold\")\n",
        "ref_sar = ref.readlines()\n",
        "ref.close()\n",
        "\n",
        "pred = open(\"model/test_0.output\")\n",
        "pred_sar = pred.readlines()\n",
        "pred.close()\n",
        "\n",
        "correct = 0\n",
        "for i in range(len(ref_sar)):\n",
        "  if ref_sar[i]==pred_sar[i]:\n",
        "    correct+=1\n",
        "\n",
        "100*correct/len(ref_sar)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "77.8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l_2F79ey3se"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeHTIMPkQ3Y4"
      },
      "source": [
        "## Single NL prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfpi_W0EdNvZ"
      },
      "source": [
        "def single_example_to_feature(example, tokenizer): # MH: make it encoder_tokenizer, decoder_tokenizer\n",
        "    features = []\n",
        "    source_tokens = tokenizer.tokenize(example)[:args.max_source_length-2]\n",
        "    source_tokens =[tokenizer.cls_token]+source_tokens+[tokenizer.sep_token]\n",
        "    source_ids =  tokenizer.convert_tokens_to_ids(source_tokens) \n",
        "    source_mask = [1] * (len(source_tokens))\n",
        "    padding_length = args.max_source_length - len(source_ids)\n",
        "    source_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "    source_mask+=[0]*padding_length\n",
        " \n",
        "    target_tokens = ['None']\n",
        "    target_tokens = [tokenizer.cls_token]+target_tokens+[tokenizer.sep_token]            \n",
        "    # target_ids = tokenizer.convert_tokens_to_ids(target_tokens) # MH: decoder\n",
        "    target_ids = decoder_tokenizer.convert_string_to_ids(' '.join(target_tokens))\n",
        "    target_mask = [1] *len(target_ids)\n",
        "    padding_length = args.max_target_length - len(target_ids)\n",
        "    target_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "    target_mask+=[0]*padding_length   \n",
        "\n",
        "    features.append(\n",
        "        InputFeatures(\n",
        "              0,\n",
        "              source_ids,\n",
        "              target_ids,\n",
        "              source_mask,\n",
        "              target_mask,\n",
        "        )\n",
        "    )\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJPiYhLiIWgK"
      },
      "source": [
        "args.eval_batch_size = 1\n",
        "args.do_test=True\n",
        "args.do_train=False\n",
        "args.beam_size=1\n",
        "args.load_model_path=output_dir+\"/checkpoint-best-bleu/pytorch_model.bin\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogRkbfcwfDph"
      },
      "source": [
        "model.eval() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldJhnZ92w3aP"
      },
      "source": [
        "def get_sar(eval_example):\n",
        "  eval_features = single_example_to_feature(eval_example, tokenizer)\n",
        "  all_source_ids = torch.tensor([f.source_ids for f in eval_features], dtype=torch.long).to(device)\n",
        "  all_source_mask = torch.tensor([f.source_mask for f in eval_features], dtype=torch.long).to(device)\n",
        "  preds = model(source_ids=all_source_ids, source_mask=all_source_mask)  \n",
        "  pred_list = list(preds[0][0].cpu().numpy())\n",
        "  predicted_text = decoder_tokenizer.decode(pred_list[:pred_list.index(0)])\n",
        "  return predicted_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l40SKW1237Wr"
      },
      "source": [
        "eval_example = 'make an app with a textbox, a button named \"tweet\", and a label. When the button is pressed, set the label to textbox text..'\n",
        "\n",
        "get_sar(eval_example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6iq_DHvHBqJ"
      },
      "source": [
        "\"string0\": go\n",
        "\"string1\": back\n",
        "<complist> <textbox> <button> go </button> <button> back </button> </complist> <code> <button1clicked> <label1> <textboxtext1> </label1> </button1clicked> <button2clicked> <ball1> <color> <gray> </color> </ball1> </button2clicked> </code>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3EpBuayf0dp"
      },
      "source": [
        "get_sar(\"make an app with an accelerometer and a music player . when accelerometer is shaken play music\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKCkz3n0gFj3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}